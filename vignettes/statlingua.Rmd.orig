---
title: "statlingua"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{statlingua}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Execute the code from the vignette
# knitr::knit("vignettes/statlingua.Rmd.orig", output = "vignettes/statlingua.Rmd")
```

## Introduction

Statistical models provide powerful insights, but their output can often be dense and filled with technical jargon. Understanding coefficients, p-values, and model fit statistics requires a certain level of expertise, making it challenging to communicate these findings to a broader audience.

The **statlingua** R package aims to bridge this gap. It leverages Large Language Models (LLMs) to convert complex statistical output into straightforward, understandable, and context-aware natural language descriptions. By feeding your statistical model objects into **statlingua**, you can effortlessly produce human-readable interpretations, thereby democratizing statistical understanding for individuals with varying levels of technical expertise.

## Prerequisites

Before you begin, ensure you have the following:

1.  The **statlingua** package installed.
2.  The [**ellmer**](https://cran.r-project.org/package=ellmer) package installed, as **statlingua** uses it to interface with LLMs.
3.  Access to an LLM provider (e.g., [OpenAI](https://openai.com/api/) or [Google Gemini AI](https://ai.google.dev/gemini-api/docs)) and an associated API key. You'll need to configure your API key according to the **ellmer** package's documentation (usually by setting environment variables like `OPENAI_API_KEY` or `GOOGLE_API_KEY`).
4.  For running the examples in this vignette, you'll also need the **ISLR2** package for the `Carseats` data set.

You can install these packages from CRAN:
```R
# install.packages(c("ellmer", "ISLR2"))
```

Next, let's load **statlingua** for use:
```{r load-statlingua}
library(statlingua)
```

## Explaining Statistical Model Output

The core function in **statlingua** is `explain()`. This is a generic function that you can apply to various statistical objects (e.g., the output from `lm()` or `t.test()`).

**statlingua** works by:
1.  Taking your R statistical object.
2.  Internally summarizing the key components of this object.
3.  Constructing a specialized prompt for an LLM, including this summary and any additional context you provide.
4.  Sending this prompt to an LLM via the **ellmer** package.
5.  Returning the LLM's natural language explanation.

### A Basic Example: Linear Regression

Let's use a linear model to predict `Sales` of child car seats from various predictors in the `Carseats` data set (from the **ISLR2** package). This example is adapted from [James et al. (2023)](https://www.statlearning.com/).

First, we'll load the data and fit the model:
```{r ex-carseats-lm}
# Ensure ISLR2 is loaded or use ISLR2::Carseats
# library(ISLR2) # Or ensure ISLR2::Carseats is accessible
carseats <- ISLR2::Carseats
fm <- lm(Sales ~ . + Income:Advertising + Price:Age, data = carseats)
summary(fm)  # print a verbose summary
```

### The Power of Context

At this point, we could directly ask **statlingua** to explain the model `fm`. However, the explanations become significantly more insightful if we provide context about our data and research goals. This is done using the `context` argument in the `explain()` function.

LLMs are powerful, but they don't inherently understand the specifics of your research, the precise meaning of your variables (e.g., "CompPrice" could mean anything without context), or the units of your data unless you provide that information. The `context` argument is your way to give the LLM this crucial background.

Effective `context` can include:

* **Research Objective**: What specific question(s) are you trying to answer with your model? (e.g., "We want to understand the key drivers of car seat sales.")
* **Data Description**:
    * What do your variables represent? (e.g., "`CompPrice` is the price charged by a competitor at each location.")
    * What are their units? (e.g., "Income is in thousands of dollars.")
    * Are there any known characteristics or limitations of the data? (e.g., "The data were recorded in the 1920s," or "This is a simulated data set.")
* **Study Design Insights**: How was the data collected? (e.g., "Data from a cross-sectional survey of 400 stores.")
* **Target Audience (Implicitly)**: While not a formal parameter, if you mention your audience in the context (e.g., "explain this to a non-statistical audience"), it can help the LLM tailor its language.

By supplying such details, you guide the LLM to:

* Interpret coefficients with their **true meaning** (e.g., relating "CompPrice" to actual competitor pricing).
* Relate findings **directly to your research goals**.
* Offer more **pertinent advice** on model assumptions or limitations.
* Generate more **targeted, less generic, and ultimately more useful explanations**.

For our `Carseats` example, here's some relevant context:

```{r ex-carseats-context}
carseats_context <- "
The model uses a data set on child car seat sales at 400 different stores.
The goal is to identify factors associated with sales.
The variables are:
  * Sales: Unit sales (in thousands) at each location (the response variable).
  * CompPrice: Price charged by competitor at each location.
  * Income: Community income level (in thousands of dollars).
  * Advertising: Local advertising budget for the company at each location (in thousands of dollars).
  * Population: Population size in the region (in thousands).
  * Price: Price the company charges for car seats at each site.
  * ShelveLoc: A factor with levels 'Bad', 'Good', and 'Medium' indicating the quality of the shelving location.
  * Age: Average age of the local population.
  * Education: Education level at each location.
  * Urban: A factor ('No', 'Yes') indicating if the store is in an urban or rural location.
  * US: A factor ('No', 'Yes') indicating if the store is in the US.
The data set is simulated.
"
```

### Getting the Explanation

Now, let's initialize an **ellmer** client (here, using Google Gemini) and call `explain()` with our model object (`fm`) and the `carseats_context`.

**Note on API Keys:** The following code assumes you have set up your Google API key for `ellmer`. Please refer to the **ellmer** documentation for details on API key configuration.

```{r ex-carseats-lm-explain, message=FALSE}
# Initialize the LLM client (ensure your API key is configured)
# The 'echo="none"' argument suppresses printing of prompts/responses by ellmer itself
# as statlingua will manage this.
client <- ellmer::chat_google_gemini(echo = "none")

# Get the explanation
explanation_output <- explain(fm, client = client, context = carseats_context)

# By default, explain() returns a character string.
# We'll print the first few characters to see.
# The full explanation can be long.
cat(substr(explanation_output, 1, 200), "...\n")
```

The explanation from the LLM can vary slightly each time you run it.

### Displaying the Explanation

The `explain()` function returns a single character string. This string is often formatted by the LLM using Markdown, which includes special characters for structure (like newline characters `\n` for paragraphs and headings, and spaces for lists).

For better readability in the R console, it's often best to pass the output of `explain()` into R's built-in `cat()` function. This will interpret the Markdown formatting (like line breaks). This is also essential for rendering the output correctly in R Markdown documents.

Alternatively, **statlingua** provides a `concatenate = TRUE` argument within the `explain()` function for convenience. Setting this will cause `explain()` to print the formatted explanation directly to the console (similar to using `cat()`) and return the explanation string invisibly.

The following two calls are effectively equivalent in how they display the output:

```{r ex-carseats-lm-explain-cat-eval, eval=FALSE}
# Option 1: Using cat()
explain(fm, client = client, context = carseats_context) |> cat()

# Option 2: Using concatenate = TRUE
explain(fm, client = client, context = carseats_context, concatenate = TRUE)
```

Here's the actual formatted output using `concatenate = TRUE` (the LLM's detailed explanation will appear between the horizontal rules):

---

```{r ex-carseats-lm-explain-cat-hide, echo=FALSE, results="asis"}
# This chunk generates and displays the actual LLM output in the vignette
# For reproducibility in a vignette, one might consider caching this result
# or saving the output to a file and reading it back in,
# as LLM calls can take time and might vary.
# For this example, we regenerate it.
explain(fm, client = client, context = carseats_context, concatenate = TRUE)
```

---

### Inspecting the LLM Interaction

If you want to see the exact prompts sent to the LLM and the raw response, you can print the `client` object that you passed to `explain()`. After a chat, the **ellmer** client object stores:

1.  The system prompt (which **statlingua** sets to guide the LLM's role and response format).
2.  The user query (constructed by **statlingua** from your model output and context).
3.  The LLM's response.

```{r ex-carseats-client, comment=""}
# The client object now contains the history of the last interaction
print(client)
```

### Follow-up Questions

A powerful feature of using LLMs is their ability to engage in conversation. If the initial explanation sparks further questions, or if you want clarification on a specific part, you can continue the conversation using the same `client` object. Simply use its `$chat()` method with your follow-up question.

For example, to ask for more details about R-squared from the previous explanation:
```{r ex-carseats-client-query, results="asis"}
msg <- "Elaborate further on the meaning of R-squared in this example, specifically for the Carseats model."
# The client remembers the context of the previous interaction
client$chat(msg) |> cat()
```

## Conclusion

The **statlingua** package offers a novel way to make statistical outputs more accessible by leveraging the natural language processing capabilities of Large Language Models. By providing clear context along with your R model objects, you can obtain detailed and understandable explanations.

For more information, please refer to the documentation for individual functions like `help(explain)`. If you encounter any issues or have suggestions, please consider reporting them on the package's development site (e.g., GitHub).
