[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"/TODO.html","id":"todo","dir":"","previous_headings":"","what":"TODO","title":"NA","text":"Systematically add support popular statistical modeling packages (e.g., brms, rstanarm, tidymodels workflows, specific time series models, glmmTMB). Allow users request explanations specific parts summary output (e.g., “explain just random effects table,” “explain ANOVA table fixed effects,” “explain specific coefficients”). Allow users supply complete prompt templates modify existing ones without altering package installation (e.g., via argument like prompt_template_dir functions export/view/load modified default prompts). Implement robust validation LLM’s output valid JSON style = \"json\" used. Define structured predictable JSON schema model type make JSON output reliable programmatic extraction information. Consider option return explanation structured R list directly, just JSON string. Provide helper functions examples easily integrating statlingua explanations R Markdown, Quarto, reporting tools. Ensure style = \"latex\" style = \"html\" fragments clean directly usable. Explore creating helper functions like to_quarto_block() to_rmd_chunk() wrap explanations appropriately.","code":""},{"path":"/articles/examples.html","id":"explaining-the-output-from-a-two-sample-t-test","dir":"Articles","previous_headings":"","what":"Explaining the output from a two-sample t-test","title":"examples","text":"following example taken tutorial paired t-tests. use independent two-sample t-test (inappropriately) analyze paired data. data: exam_scoresexam1scoreandexamscoresexam_1_score exam_scoresexam_2_score t = -0.33602, df = 27.307, p-value = 0.7394 alternative hypothesis: true difference means equal 0 95 percent confidence interval: -9.322782 6.697782 sample estimates: mean x mean y 78.1250 79.4375 ’s explanation Welch Two Sample t-test output, given context ’ve provided.","code":"library(statlingua)  # Define additional context to pass to `explain()`; this should include  # any additional background information about the data and research question. context <- \" An instructor wants to use two exams in her classes next year. This year, she gives both exams to the students. She wants to know if the exams are equally difficult and wants to check this by comparing the two sets of scores. Here is the data:   student exam_1_score exam_2_score      Bob           63           69     Nina           65           65      Tim           56           62     Kate          100           91   Alonzo           88           78     Jose           83           87   Nikhil           77           79    Julia           92           88    Tohru           90           85  Michael           84           92     Jean           68           69    Indra           74           81    Susan           87           84    Allen           64           75     Paul           71           84   Edwina           88           82 \"  # Create the data set exam_scores <- tibble::tribble(   ~student,  ~exam_1_score, ~exam_2_score,   \"Bob\",     63,            69,   \"Nina\",    65,            65,   \"Tim\",     56,            62,   \"Kate\",    100,           91,   \"Alonzo\",  88,            78,   \"Jose\",    83,            87,   \"Nikhil\",  77,            79,   \"Julia\",   92,            88,   \"Tohru\",   90,            85,   \"Michael\", 84,            92,   \"Jean\",    68,            69,   \"Indra\",   74,            81,   \"Susan\",   87,            84,   \"Allen\",   64,            75,   \"Paul\",    71,            84,   \"Edwina\",  88,            82 )  # Run a two-sample t-test (tt <- t.test(exam_scores$exam_1_score, y = exam_scores$exam_2_score)) Welch Two Sample t-test # Inititalize client client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". ex <- explain(tt, client = client, context = context) cat(ex)"},{"path":"/articles/examples.html","id":"summary-of-the-statistical-test","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"1. Summary of the Statistical Test","title":"examples","text":"Name test: Welch Two Sample t-test (also known Independent Samples t-test unequal variances). Purpose: test used compare means two independent groups. case, instructor using compare average score exam 1 average score exam 2. goal determine ’s statistically significant difference mean scores two exams. two samples (exam scores exam 1 exam 2) independent. data group approximately normally distributed. variances two groups assumed equal (’s key difference Welch’s t-test Student’s t-test).","code":""},{"path":"/articles/examples.html","id":"appropriateness-of-the-statistical-test","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"2. Appropriateness of the Statistical Test","title":"examples","text":"Given instructor’s research question data available, Welch Two Sample t-test appears reasonable choice. instructor wants compare difficulty two exams comparing scores, t-test designed comparing means. Welch’s version used don’t know variance exam scores two exams. test inappropriate assumptions met.","code":""},{"path":"/articles/examples.html","id":"suggestions-for-checking-assumptions-of-the-statistical-test","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"3. Suggestions for Checking Assumptions of the Statistical Test","title":"examples","text":"drawing firm conclusions, ’s crucial check assumptions t-test. ’s : Independence: assumption relies study design. Since student took exams, treating set exam scores separate samples, assumption might met. paired t-test might appropriate , scores student different exams independent. Histograms: Create histograms exam_1_score exam_2_score separately. Look bell-shaped distributions roughly symmetrical. Large deviations normality (skewness, multiple peaks) concern. Q-Q Plots: Create quantile-quantile plots (Q-Q plots) sets exam scores. data normally distributed, points Q-Q plot fall approximately along straight line. Deviations line indicate departures normality. Use function qqnorm() R check normality graphically. Shapiro-Wilk Test: can use Shapiro-Wilk test (shapiro.test() R) formally test normality. However, cautious interpreting results, especially small sample sizes, test can overly sensitive. Focus graphical methods first. Side--side boxplots: Compare spread (IQR, interquartile range) two boxplots. one boxplot much wider , indicates larger variance. Levene’s test: leveneTest() car package R provides formal test equality variances.","code":""},{"path":"/articles/examples.html","id":"interpretation-of-the-output","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"4. Interpretation of the Output","title":"examples","text":"Let’s break output: t = -0.33602: calculated t-statistic. represents difference sample means relative variability within samples. value 0, stronger evidence null hypothesis. df = 27.307: degrees freedom. Welch’s t-test assume equal variances, degrees freedom calculated complex way Student’s t-test. degrees freedom used determine p-value. p-value = 0.7394: probability observing t-statistic extreme , extreme , -0.33602 truly difference mean exam scores (.e., null hypothesis true). words, even two exams equally difficult, ’s 73.94% chance ’d see difference sample means big (bigger ) one observed sample. alternative hypothesis: true difference means equal 0: states alternative hypothesis tested: population means two exam scores different. two-sided test, meaning ’s looking differences either direction (exam 1 harder exam 2 harder). 95 percent confidence interval: -9.322782  6.697782: provides range plausible values true difference population means (mean exam 1 minus mean exam 2). 95% confident true difference means lies -9.32 6.70. interval includes 0, suggests difference zero (.e., difference means) plausible value. units exam score points. mean x = 78.1250: sample mean exam 1 scores. mean y = 79.4375: sample mean exam 2 scores.","code":""},{"path":"/articles/examples.html","id":"overall-conclusion","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"5. Overall Conclusion","title":"examples","text":"Using significance level α=0.05\\alpha = 0.05, since p-value (0.7394) greater α\\alpha, fail reject null hypothesis. insufficient evidence conclude statistically significant difference mean scores two exams. Based analysis, instructor enough evidence say one exam significantly harder .","code":""},{"path":"/articles/examples.html","id":"caution","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"6. Caution","title":"examples","text":"explanation generated Large Language Model. Critically review output consult additional statistical resources experts ensure correctness full understanding. Remember check assumptions test ensure validity conclusions. Furthermore, given scores student obtained tests, assumption independence may hold. paired t-test may appropriate.","code":""},{"path":"/articles/statlingua.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"The statlingua Package","text":"Statistical models indispensable tools extracting insights data, yet outputs can often cryptic laden technical jargon. Deciphering coefficients, p-values, confidence intervals, various model fit statistics typically requires solid statistical background. can create barrier communicating findings wider audience even still developing statistical acumen. statlingua R package change ! masterfully leverages power Large Language Models (LLMs) translate complex statistical model outputs clear, understandable, context-aware natural language. simply feeding R statistical model objects statlingua, can generate human-readable interpretations, making statistical understanding accessible everyone, regardless technical expertise. ’s important note statlingua doesn’t directly call LLM APIs. Instead, serves sophisticated prompt engineering toolkit. meticulously prepares necessary inputs (model summary contextual information) passes ellmer package, handles actual communication LLM. primary workhorse function ’ll use statlingua explain(). vignette guide understanding using statlingua effectively.","code":""},{"path":"/articles/statlingua.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"The statlingua Package","text":"diving , please ensure following: statlingua package installed GitHub (yet available CRAN): ellmer package installed. statlingua relies LLM communication: Access LLM provider (e.g., OpenAI, Google AI Studio, Anthropic) corresponding API key. ’ll need configure API key according ellmer package’s documentation. usually involves setting environment variables like OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY. Note ellmer supports numerous LLM providers, vignette specifically use Google Gemini models via ellmer::chat_google_gemini(); find Google Gemini particularly well-suited explaining statistical output offer generous free tier. ’ll need configure API key according ellmer package’s documentation. typically involves setting GEMINI_API_KEY environment variable R session .Renviron file (e.g., Sys.setenv(GEMINI_API_KEY = \"YOUR_API_KEY_HERE\")). examples vignette, ’ll also need following packages: ISLR2, MASS, survival.","code":"if (!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"bgreenwell/statlingua\") install.packages(\"ellmer\") install.packages(c(\"ISLR2\", \"jsonlite\", \"lme4\", \"MASS\", \"survival\"))"},{"path":"/articles/statlingua.html","id":"how-statlingua-works-the-explain-function-and-ellmer","dir":"Articles","previous_headings":"","what":"How statlingua Works: The explain() Function and ellmer","title":"The statlingua Package","text":"primary function ’ll use ****statlingua**** explain(). S3 generic function, meaning behavior adapts class R statistical object provide (e.g., \"lm\" object, \"glm\" object, \"lmerMod\" object, etc.). process explain() follows generate interpretation involves several key steps: Input & Initial Argument Resolution: call explain() statistical object, ellmer client, optionally context, audience, verbosity, new style argument. explain() generic function first resolves audience, verbosity, style specific chosen values (e.g., audience = \"novice\", style = \"markdown\") using match.arg(). resolved values passed appropriate S3 method class object. Model Summary Extraction: Internally, explain() (typically via .explain_core() helper function directly explain.default()) uses summarize() function capture text-based summary statistical object. captured text (e.g., similar summary(object) produce) forms core statistical information LLM interpret. System Prompt Assembly (via .assemble_sys_prompt()): statlingua constructs detailed instructions LLM. internal .assemble_sys_prompt() function pieces together several components, read .md files stored within package’s inst/prompts/ directory. final system prompt typically includes following sections, ordered guide LLM effectively: base role description read inst/prompts/common/role_base.md. available, model-specific role details appended inst/prompts/models/<model_name>/role_specific.md (<model_name> corresponds class object, like “lm” “lmerMod”). file doesn’t exist specific model, part omitted. Instructions tailored specified audience (e.g., \"novice\", \"researcher\") read inst/prompts/audience/<audience_value>.md (e.g., inst/prompts/audience/novice.md). Instructions defining verbosity level (e.g., \"brief\", \"detailed\") read inst/prompts/verbosity/<verbosity_value>.md (e.g., inst/prompts/verbosity/detailed.md). crucial part determined style argument. Instructions desired output format (e.g., \"markdown\", \"html\", \"json\", \"text\", \"latex\") read inst/prompts/style/<style_value>.md (e.g., inst/prompts/style/markdown.md). tells LLM structure entire response. Detailed instructions aspects statistical model explain read inst/prompts/models/<model_name>/instructions.md (e.g., \"lm\" object, read inst/prompts/models/lm/instructions.md). model-specific instructions aren’t found, defaults inst/prompts/models/default/instructions.md. general caution message appended inst/prompts/common/caution.md. components assembled single, comprehensive system prompt guides LLM’s behavior, tone, content focus, output format. User Prompt Construction (via .build_usr_prompt()): “user prompt” (actual query containing data interpreted) constructed combining: leading phrase indicating type model (e.g., “Explain following linear regression model output:”). captured model output_summary step 2. additional context string provided user via context argument. LLM Interaction via ellmer: assembled sys_prompt set ellmer client object. , constructed usr_prompt sent LLM using client$chat(usr_prompt). ellmer handles actual API communication. Output Post-processing (via .remove_fences()): returning explanation, ****statlingua**** calls internal utility, .remove_fences(), clean LLM’s raw output. function attempts remove common “language fence” wrappers (like markdown ... json ...) LLMs sometimes add around responses. Output Packaging: cleaned explanation string LLM packaged statlingua_explanation object. object’s text component holds explanation string specified style. also includes metadata like model_type, audience, verbosity, style used. statlingua_explanation object default print method uses cat() easy viewing console. comprehensive modular approach prompt engineering allows statlingua provide tailored well-formatted explanations variety statistical models user needs.","code":""},{"path":"/articles/statlingua.html","id":"understanding-explains-arguments","dir":"Articles","previous_headings":"How statlingua Works: The explain() Function and ellmer","what":"Understanding explain()’s Arguments","title":"The statlingua Package","text":"explain() function flexible, several arguments fine-tune behavior: object: primary input – R statistical object (e.g., \"lm\" model, \"glm\" model, output t.test(), coxph(), etc.). client: Essential. ellmer client object (e.g., created ellmer::chat_google_gemini()). statlingua uses communicate LLM. must initialize configure client API key beforehand. context (Optional Highly Recommended): character string providing background information data, research questions, variable definitions, units, study design, etc. Default NULL. audience (Optional): Specifies target audience explanation. Options include: \"novice\" (default), \"student\", \"researcher\", \"manager\", \"domain_expert\". verbosity (Optional): Controls level detail. Options : \"moderate\" (default), \"brief\", \"detailed\". \"markdown\": Output formatted Markdown. \"html\": Output formatted HTML fragment. \"json\": Output structured JSON string parseable R list (see example parsing). \"text\": Output plain text. \"latex\": Output LaTeX fragment. ... (Optional): Additional optional arguments (currently ignored statlingua’s explain methods).","code":""},{"path":"/articles/statlingua.html","id":"the-power-of-context-why-it-matters","dir":"Articles","previous_headings":"","what":"The Power of context: Why It Matters","title":"The statlingua Package","text":"just pass model object explain() get basic interpretation. However, unlock truly insightful actionable explanations, providing context paramount. LLMs incredibly powerful, don’t inherently know nuances specific research. don’t know “VarX” really means data set, units, specific hypothesis ’re testing, population ’re studying unless tell . context argument channel provide vital background. makes effective context? Research Objective: question(s) trying answer? (e.g., “investigating factors affecting Gentoo penguin bill length understand dietary adaptations.”) variables represent? specific. (e.g., “bill_length_mm length penguin’s bill millimeters.”) units? (e.g., “Flipper length millimeters, body mass grams.”) known data limitations special characteristics? (e.g., “Data collected three islands Palmer Archipelago.”) Study Design: data collected? (e.g., “Observational data field study.”) Target Audience Nuances (Implicitly): audience argument handles main targeting, mentioning specific interpretation needs context can refine LLM’s output (e.g., “Explain practical significance findings wildlife conservation efforts.”). supplying details, empower LLM : Interpret coefficients true, domain-specific meaning. Relate findings directly research goals. Offer relevant advice model assumptions limitations. Generate explanations less generic, targeted, ultimately far useful. Think context difference asking generic statistician “mean?” versus asking statistician deeply understands research area, data, objectives. latter always provide valuable interpretation.","code":""},{"path":"/articles/statlingua.html","id":"some-examples-in-action","dir":"Articles","previous_headings":"","what":"Some Examples in Action!","title":"The statlingua Package","text":"Let’s see statlingua shine practical examples. Important Note API Keys: following code chunks call explain() set eval = FALSE default vignette. require active API key configured ellmer. run examples : Ensure API key (e.g., GOOGLE_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY) set environment variable ellmer can access. Change R chunk option eval = FALSE eval = TRUE chunks wish run. may need adjust ellmer client initialization (e.g., ellmer::chat_openai()) match chosen LLM provider. examples vignette","code":""},{"path":"/articles/statlingua.html","id":"example-1-linear-regression-lm---sales-of-child-car-seats","dir":"Articles","previous_headings":"Some Examples in Action!","what":"Example 1: Linear Regression (lm) - Sales of Child Car Seats","title":"The statlingua Package","text":"Let’s use linear model predict Sales child car seats various predictors using Carseats data set package ISLR2. make example bit complicated, ’ll include pairwise interaction effects model (can include polynomial terms, smoothing splines, type transformation makes sense). Note categorical variables ShelveLoc, Urban, US dummy encoded default). next code chunk loads statlingua package establishes connection (default) Google Gemini model. also define context LLM use explaining output: Next, let’s use Google Gemini model generate explanation model’s output, targeting \"student\" audience \"detailed\" verbosity.","code":"data(Carseats, package = \"ISLR2\")  # load the Carseats data  # Fit a linear model to the Carseats data set fm_carseats <- lm(Sales ~ . + Price:Age + Income:Advertising, data = Carseats) summary(fm_carseats)  # print model summary #>  #> Call: #> lm(formula = Sales ~ . + Price:Age + Income:Advertising, data = Carseats) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -2.9208 -0.7503  0.0177  0.6754  3.3413  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)         6.5755654  1.0087470   6.519 2.22e-10 *** #> CompPrice           0.0929371  0.0041183  22.567  < 2e-16 *** #> Income              0.0108940  0.0026044   4.183 3.57e-05 *** #> Advertising         0.0702462  0.0226091   3.107 0.002030 **  #> Population          0.0001592  0.0003679   0.433 0.665330     #> Price              -0.1008064  0.0074399 -13.549  < 2e-16 *** #> ShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 *** #> ShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 *** #> Age                -0.0579466  0.0159506  -3.633 0.000318 *** #> Education          -0.0208525  0.0196131  -1.063 0.288361     #> UrbanYes            0.1401597  0.1124019   1.247 0.213171     #> USYes              -0.1575571  0.1489234  -1.058 0.290729     #> Price:Age           0.0001068  0.0001333   0.801 0.423812     #> Income:Advertising  0.0007510  0.0002784   2.698 0.007290 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 1.011 on 386 degrees of freedom #> Multiple R-squared:  0.8761, Adjusted R-squared:  0.8719  #> F-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16 library(statlingua)  # Establish client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\".  # Additional context for the LLM to consider carseats_context <- \" The model uses a data set on child car seat sales (in thousands of units) at 400 different stores. The goal is to identify factors associated with sales. The variables are:   * Sales: Unit sales (in thousands) at each location (the response variable).   * CompPrice: Price charged by competitor at each location.   * Income: Community income level (in thousands of dollars).   * Advertising: Local advertising budget for the company at each location (in thousands of dollars).   * Population: Population size in the region (in thousands).   * Price: Price the company charges for car seats at each site.   * ShelveLoc: A factor with levels 'Bad', 'Good', and 'Medium' indicating the quality of the shelving location for the car seats. ('Bad' is the reference level).   * Age: Average age of the local population.   * Education: Education level at each location.   * Urban: A factor ('No', 'Yes') indicating if the store is in an urban or rural location. ('No' is the reference level).   * US: A factor ('No', 'Yes') indicating if the store is in the US or not. ('No' is the reference level). Interaction terms `Income:Advertising` and `Price:Age` are also included. The data set is simulated. We want to understand key drivers of sales and how to interpret the interaction terms. \" explain(fm_carseats, client = client, context = carseats_context,         audience = \"novice\", verbosity = \"detailed\")"},{"path":"/articles/statlingua.html","id":"interpretation-of-linear-regression-model-output","dir":"Articles","previous_headings":"","what":"Interpretation of Linear Regression Model Output","title":"The statlingua Package","text":"section provides detailed interpretation linear regression model output, given context car seat sales data.","code":""},{"path":"/articles/statlingua.html","id":"model-appropriateness","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Model Appropriateness","title":"The statlingua Package","text":"Given response variable, Sales, continuous goal understand linear relationships sales various predictors, linear regression model appears reasonable choice. However, ’s important verify model’s assumptions (linearity, independence, homoscedasticity, normality errors) ensure validity results.","code":""},{"path":"/articles/statlingua.html","id":"call","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Call","title":"The statlingua Package","text":"shows R command used fit linear regression model. formula indicates Sales response variable, modeled function variables Carseats data frame, along interaction terms Price:Age Income:Advertising.","code":"lm(formula = Sales ~ . + Price:Age + Income:Advertising, data = Carseats)"},{"path":"/articles/statlingua.html","id":"residuals-summary","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Residuals Summary","title":"The statlingua Package","text":"section summarizes distribution residuals (differences observed predicted values Sales). Min: smallest residual -2.9208. means model overpredicted sales approximately 2,921 units one location. 1Q: 25% residuals less -0.7503. Thus, 25% stores, model overpredicted sales approximately 750 units. Median: median residual 0.0177, close zero. suggests residuals roughly symmetrically distributed around zero, good sign. 3Q: 75% residuals less 0.6754. Thus, 25% stores, model underpredicted sales approximately 675 units. Max: largest residual 3.3413. means model underpredicted sales approximately 3,341 units one location. Ideally, residuals symmetrically distributed around zero. , median close zero, indicating reasonably well-centered distribution.","code":"Min      1Q  Median      3Q     Max  -2.9208 -0.7503  0.0177  0.6754  3.3413"},{"path":"/articles/statlingua.html","id":"coefficients-table","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Coefficients Table","title":"The statlingua Package","text":"table presents estimated coefficients predictor model. (Intercept): estimated intercept 6.5755654. predicted sales (thousands) predictor variables zero. practice, value might meaningful, ’s unlikely predictors zero simultaneously. CompPrice: estimated coefficient 0.0929371. every $1 increase competitor’s price, company’s sales predicted increase approximately 0.093 thousand units (93 units), holding variables constant. p-value less 2e-16, highly significant, suggesting strong positive relationship competitor price sales. Income: estimated coefficient 0.0108940. every $1,000 increase community income, company’s sales predicted increase approximately 0.011 thousand units (11 units), holding variables constant. p-value 3.57e-05, highly significant, suggesting strong positive relationship community income sales. Advertising: estimated coefficient 0.0702462. every $1,000 increase local advertising budget, company’s sales predicted increase approximately 0.070 thousand units (70 units), holding variables constant. p-value 0.002030, significant, suggesting positive relationship advertising sales. Population: estimated coefficient 0.0001592. every 1,000-person increase population size, company’s sales predicted increase approximately 0.00016 thousand units (0.16 units), holding variables constant. p-value 0.665330, significant, suggesting population size strong predictor sales model. Price: estimated coefficient -0.1008064. every $1 increase company’s price, company’s sales predicted decrease approximately 0.101 thousand units (101 units), holding variables constant. p-value less 2e-16, highly significant, suggesting strong negative relationship price sales. ShelveLocGood: estimated coefficient 4.8486762. store “Good” shelving location predicted approximately 4.849 thousand units (4,849 units) higher sales store “Bad” shelving location (reference level), holding variables constant. p-value less 2e-16, highly significant, suggesting strong positive impact good shelving location sales. ShelveLocMedium: estimated coefficient 1.9532620. store “Medium” shelving location predicted approximately 1.953 thousand units (1,953 units) higher sales store “Bad” shelving location (reference level), holding variables constant. p-value less 2e-16, highly significant, suggesting strong positive impact medium shelving location sales, though less impactful “Good” location. Age: estimated coefficient -0.0579466. every one-year increase average age local population, company’s sales predicted decrease approximately 0.058 thousand units (58 units), holding variables constant. p-value 0.000318, highly significant, suggesting negative relationship age sales. Education: estimated coefficient -0.0208525. every one-year increase average education level location, company’s sales predicted decrease approximately 0.021 thousand units (21 units), holding variables constant. p-value 0.288361, significant, suggesting education level strong predictor sales model. UrbanYes: estimated coefficient 0.1401597. store urban area predicted approximately 0.140 thousand units (140 units) higher sales store rural area (reference level), holding variables constant. p-value 0.213171, significant, suggesting urban/rural location strong predictor sales model. USYes: estimated coefficient -0.1575571. store US predicted approximately 0.158 thousand units (158 units) lower sales store US (reference level), holding variables constant. p-value 0.290729, significant, suggesting US/non-US location strong predictor sales model. Price:Age: estimated coefficient 0.0001068. interaction effect Price Age. means effect Price Sales depends Age, vice versa. Specifically, every one-year increase age, effect price sales becomes less negative 0.0001068 thousand units. p-value 0.423812, significant, suggesting interaction effect statistically strong model. Income:Advertising: estimated coefficient 0.0007510. interaction effect Income Advertising. means effect Income Sales depends Advertising, vice versa. Specifically, every $1,000 increase advertising budget, effect income sales becomes positive 0.0007510 thousand units. p-value 0.007290, significant, suggesting interaction effect statistically meaningful model.","code":"Estimate Std. Error t value Pr(>|t|)     (Intercept)         6.5755654  1.0087470   6.519 2.22e-10 *** CompPrice           0.0929371  0.0041183  22.567  < 2e-16 *** Income              0.0108940  0.0026044   4.183 3.57e-05 *** Advertising         0.0702462  0.0226091   3.107 0.002030 **  Population          0.0001592  0.0003679   0.433 0.665330     Price              -0.1008064  0.0074399 -13.549  < 2e-16 *** ShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 *** ShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 *** Age                -0.0579466  0.0159506  -3.633 0.000318 *** Education          -0.0208525  0.0196131  -1.063 0.288361     UrbanYes            0.1401597  0.1124019   1.247 0.213171     USYes              -0.1575571  0.1489234  -1.058 0.290729     Price:Age           0.0001068  0.0001333   0.801 0.423812     Income:Advertising  0.0007510  0.0002784   2.698 0.007290 **"},{"path":"/articles/statlingua.html","id":"signif--codes","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Signif. codes","title":"The statlingua Package","text":"section explains significance codes used coefficients table: ***: Highly significant (p-value < 0.001) **: Significant (p-value < 0.01) *: Moderately significant (p-value < 0.05) .: Marginally significant (p-value < 0.1) (blank): significant (p-value >= 0.1)","code":"Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"/articles/statlingua.html","id":"residual-standard-error","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Residual Standard Error","title":"The statlingua Package","text":"residual standard error (RSE) 1.011. represents typical size prediction errors. words, observed sales values typically deviate predicted sales values approximately 1.011 thousand units (1,011 units). degrees freedom 386, calculated number observations (400) minus number estimated coefficients (14).","code":"Residual standard error: 1.011 on 386 degrees of freedom"},{"path":"/articles/statlingua.html","id":"r-squared","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"R-squared","title":"The statlingua Package","text":"Multiple R-squared: value (0.8761) indicates approximately 87.61% variance Sales explained predictor variables included model. Adjusted R-squared: modified version R-squared adjusts number predictors model. generally preferred comparing models different numbers predictors. case, adjusted R-squared 0.8719, 87.19%. suggests model explains substantial portion variability sales, even accounting complexity model.","code":"Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719"},{"path":"/articles/statlingua.html","id":"f-statistic","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"F-statistic","title":"The statlingua Package","text":"F-statistic tests overall significance model. F-statistic 210, 13 386 degrees freedom. associated p-value less 2.2e-16, extremely small. indicates least one predictor variables significantly related Sales. words, model whole significantly better predicting sales model intercept.","code":"F-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16"},{"path":"/articles/statlingua.html","id":"suggestions-for-checking-assumptions","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Suggestions for Checking Assumptions","title":"The statlingua Package","text":"ensure validity linear regression model, ’s crucial check key assumptions: Linearity: Create scatterplot residuals versus fitted values. Look random scatter points around zero. see pattern (e.g., curve), suggests non-linearity. suspect specific predictor non-linear, plot residuals predictor. Independence Errors: data time component spatial structure, consider plotting residuals time space order check patterns. Durbin-Watson test can used formally test autocorrelation. Homoscedasticity (Constant Variance Errors): Examine residuals versus fitted values plot. Look constant variance residuals across range fitted values. Also, create Scale-Location plot (square root standardized residuals vs. fitted values) help assess homoscedasticity. Formal tests like Breusch-Pagan White test can also used. Normality Errors: Create Normal Q-Q plot residuals. residuals normally distributed, points fall approximately along straight line. Also, create histogram density plot residuals visually check looks approximately normal. Shapiro-Wilk test can used formally test normality, visual inspection Q-Q plot histogram often informative. Multicollinearity: Calculate Variance Inflation Factors (VIFs) predictor. High VIF values (e.g., greater 5 10) suggest multicollinearity, can inflate standard errors make difficult interpret individual coefficients. Influential Observations: Create “Residuals vs Leverage” plot. Look points high leverage (potential influence regression coefficients) large residuals (poorly predicted). points may influential warrant investigation. Cook’s distance can also used identify influential points. Important Note: explanation generated Large Language Model. Critically review output consult additional statistical resources experts ensure correctness full understanding, especially given interpretation relies provided output context.","code":""},{"path":"/articles/statlingua.html","id":"follow-up-question-interpreting-r-squared","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output > Suggestions for Checking Assumptions","what":"Follow-up Question: Interpreting R-squared","title":"The statlingua Package","text":"initial explanation great, let’s say student wants understand R-squared deeply particular model. can use $chat() method client (ellmer \"Chat\" object), remembers context previous interaction. LLM provided detailed explanation R-squared, tailored fm_carseats model provided context, discussing much variability Sales explained predictors model.","code":"query <- paste(\"Could you explain the R-squared values (Multiple R-squared and\",                \"Adjusted R-squared) in simpler terms for this car seat sales\",                \"model? What does it practically mean for predicting sales?\") client$chat(query) #> [1] \"## Simplified Explanation of R-squared for Car Seat Sales Model\\n\\nLet's break down R-squared in a way that's easy to understand, specifically for this car seat sales model.\\n\\n### What is R-squared?\\n\\nImagine you're trying to guess the sales of car seats at a store. Without any information, your guess would be pretty inaccurate. R-squared tells you how much better you can guess *with* the information from our model (like price, advertising, location, etc.) compared to guessing blindly.\\n\\nR-squared is a number between 0 and 1 (or 0% and 100%). It represents the *proportion* of the variability in car seat sales that our model can explain or predict. Think of it like this:\\n\\n*   **R-squared = 0 (or 0%)**: The model explains *none* of the variability in sales. Knowing the price, advertising budget, location, etc., doesn't help you predict sales any better than just guessing a constant average value for all the stores. Our model is useless.\\n\\n*   **R-squared = 1 (or 100%)**: The model explains *all* of the variability in sales. If you know the price, advertising budget, location, etc., you can perfectly predict sales for every store, every time. Our model is perfect.\\n\\n*   **R-squared = something in between (like our case)**: The model explains *some* of the variability in sales. The higher the R-squared, the better the model is at predicting sales.\\n\\n### Multiple R-squared\\n\\nThe *Multiple R-squared* in our model output is 0.8761 (or 87.61%). This means that *about 87.6% of the variation in car seat sales across different stores can be explained by the factors included in our model* (competitor price, income, advertising, price, shelving location, age, urban/rural location, US/non-US location, and the interaction terms).\\n\\nIn simpler terms, knowing these factors allows us to predict sales much more accurately than if we didn't know them. We've captured a large proportion of what causes sales to go up or down.\\n\\n### Adjusted R-squared\\n\\nThe *Adjusted R-squared* is 0.8719 (or 87.19%). It's very close to the Multiple R-squared, but it's a bit lower. The Adjusted R-squared is a *more realistic* measure of how well our model will predict sales on *new* data.\\n\\nHere's why it's adjusted: When you add more predictors to a model, the R-squared will always go up (even if those new predictors are just random noise). The Adjusted R-squared penalizes us for adding predictors that don't really improve the model. It accounts for the number of predictors in the model.\\n\\nThink of it like this:\\n* Imagine you are assembling a basketball team.\\n* You have a very simple model to predict wins based on the height of players. This gets you some of the way there but you would not win all the games.\\n* If you add many new, useless players to the team who don't add any value, the adjusted R-squared will penalize you, because adding many players doesn't automatically make the team (model) better.\\n* If you add players who actually make the team significantly better, then the adjusted R-squared will reflect that.\\n\\n**In our case:** The Adjusted R-squared is only slightly lower than the Multiple R-squared, suggesting the predictors are useful and not just inflating the R-squared value.\\n\\n### What Does it Mean Practically for Predicting Sales?\\n\\nAn R-squared of around 0.87 (or 87%) is generally considered quite good. It suggests that our model is reasonably effective at predicting car seat sales.\\n\\n**Here's what it means in practice:**\\n\\n*   **Making Better Decisions:** We can use this model to make better business decisions. For example, we can see that *Price* has a negative impact on sales and *ShelveLocGood* has a positive impact. With the size of the estimated coefficients, we can evaluate how a price change or change to shelf location may affect sales.\\n*   **Resource Allocation:** The model helps us prioritize. Advertising combined with Income seems to be important. The model can help us decide where to focus advertising dollars.\\n*   **Store Performance Evaluation:** We can compare a store's actual sales to the sales predicted by the model. Stores that are significantly underperforming relative to their predicted sales might need attention.\\n*   **Forecasting:** We can use the model to forecast future sales, given expected values for the predictor variables (like income, competitor price, etc.).\\n\\n**Important Caveats:**\\n\\n*   **Not Perfect:** Even with a high R-squared, our model is not perfect. There's still about 13% of the variation in sales that it *doesn't* explain. Other factors not included in the model could be at play (e.g., local events, seasonality, store layout, other marketing efforts, random chance).\\n*   **Association vs. Causation:** R-squared only tells us about the strength of the association between the predictors and sales. It *doesn't* prove that changes in the predictors *cause* changes in sales. There might be other underlying factors that we haven't considered.\\n*   **Assumptions:** Remember that R-squared is only meaningful if the assumptions of linear regression are reasonably met (linearity, independence of errors, homoscedasticity, normality of errors). If these assumptions are violated, the R-squared value can be misleading.\\n*   **Out-of-Sample Prediction:** An R-squared calculated on the data used to build the model (in-sample) is often an overestimate of how well the model will perform on new, unseen data (out-of-sample). It is always a good idea to test the performance of a model on out-of-sample data, if possible.\\n\\nIn summary, the R-squared values tell us that our model is a useful tool for understanding and predicting car seat sales, but it's not a crystal ball. It's important to use it in conjunction with good business sense and an understanding of the limitations.\\n\""},{"path":"/articles/statlingua.html","id":"example-2-logistic-glm-glm---pima-indians-diabetes","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Example 2: Logistic GLM (glm) - Pima Indians Diabetes","title":"The statlingua Package","text":"Let’s use Pima.tr data set MASS package fit logistic regression model. data set prevalence diabetes Pima Indian women. goal identify factors associated likelihood testing positive diabetes. Now, let’s provide additional context accompany output requesting explanation LLM: time, ’ll ask statlingua explanation, targeting \"researcher\" \"moderate\" verbosity. audience interested aspects like odds ratios model fit.","code":"data(Pima.tr, package = \"MASS\")  # load the Pima.tr data set  # Fit a logistic regression model fm_pima <- glm(type ~ npreg + glu + bp + skin + bmi + ped + age,                data = Pima.tr, family = binomial(link = \"logit\")) summary(fm_pima)  # print model summary #>  #> Call: #> glm(formula = type ~ npreg + glu + bp + skin + bmi + ped + age,  #>     family = binomial(link = \"logit\"), data = Pima.tr) #>  #> Deviance Residuals:  #>     Min       1Q   Median       3Q      Max   #> -1.9830  -0.6773  -0.3681   0.6439   2.3154   #>  #> Coefficients: #>              Estimate Std. Error z value Pr(>|z|)     #> (Intercept) -9.773062   1.770386  -5.520 3.38e-08 *** #> npreg        0.103183   0.064694   1.595  0.11073     #> glu          0.032117   0.006787   4.732 2.22e-06 *** #> bp          -0.004768   0.018541  -0.257  0.79707     #> skin        -0.001917   0.022500  -0.085  0.93211     #> bmi          0.083624   0.042827   1.953  0.05087 .   #> ped          1.820410   0.665514   2.735  0.00623 **  #> age          0.041184   0.022091   1.864  0.06228 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 256.41  on 199  degrees of freedom #> Residual deviance: 178.39  on 192  degrees of freedom #> AIC: 194.39 #>  #> Number of Fisher Scoring iterations: 5 pima_context <- \" This logistic regression model attempts to predict the likelihood of a Pima Indian woman testing positive for diabetes. The data is from a study on women of Pima Indian heritage, aged 21 years or older, living near Phoenix, Arizona. The response variable 'type' is binary: 'Yes' (tests positive for diabetes) or 'No'.  Predictor variables include:   - npreg: Number of pregnancies.   - glu: Plasma glucose concentration in an oral glucose tolerance test.   - bp: Diastolic blood pressure (mm Hg).   - skin: Triceps skin fold thickness (mm).   - bmi: Body mass index (weight in kg / (height in m)^2).   - ped: Diabetes pedigree function (a measure of genetic predisposition).   - age: Age in years.  The goal is to understand which of these factors are significantly associated with an increased or decreased odds of having diabetes. We are particularly interested in interpreting coefficients as odds ratios. \"  # Establish fresh client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". explain(fm_pima, client = client, context = pima_context,         audience = \"researcher\", verbosity = \"moderate\")"},{"path":"/articles/statlingua.html","id":"explanation-of-the-binomial-glm-with-logit-link-output","dir":"Articles","previous_headings":"","what":"Explanation of the Binomial GLM with Logit Link Output","title":"The statlingua Package","text":"output details logistic regression model, type Generalized Linear Model (GLM) used predict binary outcome. , model predicts probability Pima Indian woman testing positive diabetes (‘Yes’ vs. ‘’) based several physiological demographic predictors. binomial family logit link appropriate binary outcomes models log-odds event (diabetes diagnosis) linear combination predictors. logit link ensures predicted probabilities fall 0 1. Key Assumptions: * Observations (individual women) independent. * logit transformation probability diabetes linearly related predictors. * variance errors follows binomial distribution. Given context, choice logistic regression model appears suitable binary response variable (diabetes diagnosis). Interpretation glm() Output: Call: R command used fit model. Deviance Residuals: residuals summarize model’s goodness--fit. Ideally, symmetrically distributed around zero. range (-1.9830 2.3154) suggests reasonable fit, investigation (e.g., plotting residuals) recommended. Coefficients Table: table core interpretation. coefficient represents change log-odds testing positive diabetes one-unit increase predictor, holding predictors constant. Estimate: estimated coefficient log-odds scale. Std. Error: standard error coefficient estimate. z value: z-statistic, calculated Estimate / Std. Error. Pr(>|z|): p-value, representing probability observing z-statistic extreme , extreme , one calculated, assuming null hypothesis (coefficient = 0) true. Exponentiated Estimate (Odds Ratio): exp(Estimate) transforms coefficient log-odds scale odds ratio scale, making directly interpretable. odds ratio > 1 indicates positive association diabetes, odds ratio < 1 indicates negative association. Signif. codes: linear regression, codes indicate level statistical significance predictor. (Dispersion parameter binomial family taken 1): binomial GLM, dispersion parameter fixed 1. indicates model assumes variance equal mean (expected binomial distribution). evidence overdispersion (variance significantly greater mean), quasibinomial family considered. Since residual deviance (178.39) close residual degrees freedom (192), strong evidence overdispersion model. Null deviance: 256.41 199 degrees freedom represents deviance null model (model intercept). indicates amount variation response variable explained null model. Residual deviance: 178.39 192 degrees freedom represents deviance fitted model. indicates amount variation response variable explained model. difference null deviance residual deviance (256.41 - 178.39 = 78.02) indicates amount variation explained model. AIC: 194.39 Akaike Information Criterion (AIC) measure model fit penalizes model complexity. Lower AIC values indicate better balance model fit complexity. AIC useful comparing different models fit data. Number Fisher Scoring iterations: 5 indicates model converged 5 iterations Fisher scoring algorithm, used estimate model coefficients. Suggestions Checking Assumptions Analysis: Residual Plots: Create plots deviance residuals fitted values (linear predictor scale) check patterns non-constant variance. can help assess appropriateness link function assumed variance structure. Influential Observations: Identify examine influential observations may disproportionately affect model results. Overdispersion: Although residual deviance close residual degrees freedom, ’s still worth formally testing overdispersion. Interaction Terms: Consider adding interaction terms predictors see effect one predictor depends value another. example, interaction age BMI might relevant. Collinearity: Examine correlations predictors multicollinearity, inflate standard errors make difficult interpret individual effects predictors. Variance inflation factors (VIFs) can calculated. Caution: explanation generated Large Language Model. Critically review output consult additional statistical resources experts ensure correctness full understanding, especially given interpretation relies provided output context. (rendered Markdown) explains logistic regression coefficients (e.g., glu bmi) terms log-odds odds ratios, discusses statistical significance, interprets overall model fit statistics like AIC deviance. researcher, explanation might also touch upon implications findings diabetes risk assessment. Thank catching error! ’s crucial accurate reliable examples. Pima.tr data set much suitable choice GLM example.","code":"glm(formula = type ~ npreg + glu + bp + skin + bmi + ped + age,      family = binomial(link = \"logit\"), data = Pima.tr)"},{"path":"/articles/statlingua.html","id":"example-3-cox-proportional-hazards-model-coxph---lung-cancer-survival","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output","what":"Example 3: Cox Proportional Hazards Model (coxph) - Lung Cancer Survival","title":"The statlingua Package","text":"Let’s model patient survival lung cancer study using lung data set survival package. classic data set Cox PH models. ’s additional context provide lung cancer survival model: Let’s get explanation \"manager\" audience, looking \"brief\" overview. ’s interpretation Cox Proportional Hazards model output, focusing key findings hazard ratios.","code":"library(survival)  # Load the lung cancer data set (from package survival) data(cancer)  # Fit a time transform Cox PH model using current age fm_lung <- coxph(Surv(time, status) ~ ph.ecog + tt(age), data = lung,      tt = function(x, t, ...) pspline(x + t/365.25)) summary(fm_lung)  # print model summary #> Call: #> coxph(formula = Surv(time, status) ~ ph.ecog + tt(age), data = lung,  #>     tt = function(x, t, ...) pspline(x + t/365.25)) #>  #>   n= 227, number of events= 164  #>    (1 observation deleted due to missingness) #>  #>                 coef    se(coef) se2      Chisq DF   p       #> ph.ecog         0.45284 0.117827 0.117362 14.77 1.00 0.00012 #> tt(age), linear 0.01116 0.009296 0.009296  1.44 1.00 0.23000 #> tt(age), nonlin                            2.70 3.08 0.45000 #>  #>                    exp(coef) exp(-coef) lower .95 upper .95 #> ph.ecog                1.573     0.6358    1.2484     1.981 #> ps(x + t/365.25)3      1.275     0.7845    0.2777     5.850 #> ps(x + t/365.25)4      1.628     0.6141    0.1342    19.761 #> ps(x + t/365.25)5      2.181     0.4585    0.1160    41.015 #> ps(x + t/365.25)6      2.762     0.3620    0.1389    54.929 #> ps(x + t/365.25)7      2.935     0.3408    0.1571    54.812 #> ps(x + t/365.25)8      2.843     0.3517    0.1571    51.472 #> ps(x + t/365.25)9      2.502     0.3997    0.1382    45.310 #> ps(x + t/365.25)10     2.529     0.3955    0.1390    45.998 #> ps(x + t/365.25)11     3.111     0.3214    0.1699    56.961 #> ps(x + t/365.25)12     3.610     0.2770    0.1930    67.545 #> ps(x + t/365.25)13     5.487     0.1822    0.2503   120.280 #> ps(x + t/365.25)14     8.903     0.1123    0.2364   335.341 #>  #> Iterations: 4 outer, 10 Newton-Raphson #>      Theta= 0.7960256  #> Degrees of freedom for terms= 1.0 4.1  #> Concordance= 0.612  (se = 0.027 ) #> Likelihood ratio test= 22.46  on 5.07 df,   p=5e-04 lung_context <- \" This Cox proportional hazards model analyzes survival data for patients with advanced lung cancer. The objective is to identify factors associated with patient survival time (in days). The variables include:   - time: Survival time in days.   - status: Censoring status (1=censored, 2=dead).   - age: Age in years.   - sex: Patient's sex (1=male, 2=female). Note: In the model, 'sex' is treated as numeric; interpretations should consider this. It's common to factor this, but here it's numeric.   - ph.ecog: ECOG performance score (0=good, higher values mean worse performance). We want to understand how age, sex, and ECOG score relate to the hazard of death. Interpretations should focus on hazard ratios. For example, how does a one-unit increase in ph.ecog affect the hazard of death? \"  # Establish fresh client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". explain(fm_lung, client = client, context = lung_context,         audience = \"manager\", verbosity = \"brief\")"},{"path":"/articles/statlingua.html","id":"model-overview","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output","what":"Model Overview","title":"The statlingua Package","text":"Call: model examines effect ph.ecog time-dependent effect age (using penalized spline, pspline) survival time (time) censoring status (status) lung cancer patients. tt() function indicates time-varying effect. Data: analysis includes 227 patients, 164 observed deaths (events). One observation removed due missing data.","code":""},{"path":[]},{"path":"/articles/statlingua.html","id":"ph-ecog-ecog-performance-score","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output > Predictor Variable Interpretation","what":"1. ph.ecog (ECOG Performance Score)","title":"The statlingua Package","text":"coef: 0.45284. represents estimated increase log-hazard one-unit increase ph.ecog. positive coefficient means higher ECOG score associated higher hazard. exp(coef) (Hazard Ratio): 1.573. one-unit increase ECOG performance score, hazard death estimated 1.573 times higher, holding age constant. indicates patients higher (worse) ECOG performance score significantly increased risk death. Specifically, one-unit increase ph.ecog, risk approximately 57.3% higher ((1.573-1)*100%). Confidence Interval (95%): [1.2484, 1.981]. interval include 1, suggesting effect ph.ecog statistically significant.","code":""},{"path":"/articles/statlingua.html","id":"ttage---time-varying-effect-of-age-using-pspline","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output > Predictor Variable Interpretation","what":"2. tt(age) - Time-Varying Effect of Age (using pspline)","title":"The statlingua Package","text":"model includes time-varying effect age, using penalized spline. tt(age), linear: 0.01116 - represents linear component time-varying effect age log-hazard scale. exp(coef): directly interpretable simple hazard ratio due spline transformation. tt(age), nonlin: Chi-square test (2.70 3.08 df, p=0.45) suggests non-linear component time-varying effect age statistically significant. implies relationship age hazard may approximately linear time. exp(coef) spline terms: exponential coefficients ps(x + t/365.25)3 ps(x + t/365.25)14 represent hazard ratios relative baseline spline function. pspline, interpretation complex less direct. show change hazard ratio component spline. confidence intervals wide, implying high uncertainty. General comment time-varying effects: age effect modeled time-varying, effect age hazard changes time passes. spline terms describe change. However, given non-significant p-value non-linear term, difficult interpret spline terms directly.","code":""},{"path":"/articles/statlingua.html","id":"overall-model-evaluation","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output","what":"Overall Model Evaluation","title":"The statlingua Package","text":"Likelihood Ratio Test: likelihood ratio test (22.46 5.07 df, p=5e-04) indicates model ph.ecog time-varying age significant improvement null model (model predictors). Concordance: 0.612 (SE = 0.027). indicates approximately 61.2% patient pairs, model correctly predicts patient experience event (death) sooner. measure model’s ability discriminate patients different survival times. Concordance values range 0.5 1.0, 0.5 indicates random prediction, 1.0 indicates perfect prediction.","code":""},{"path":"/articles/statlingua.html","id":"recommendations-and-cautions","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output","what":"Recommendations and Cautions","title":"The statlingua Package","text":"Proportional Hazards Assumption: essential assess proportional hazards assumption, especially given use Cox model. can done using Schoenfeld residuals via cox.zph() function R. Significant p-values non-random patterns plots Schoenfeld residuals versus time indicate violations PH assumption. Given time-varying coefficient age, careful checking even important. Time-Varying Age Interpretation: effect age complex due time-varying nature. model assumes effect age hazard changes time. investigation needed fully understand nature interaction. non-linear effect age significant, consider removing pspline adding linear effect age. Spline Terms: Interpret pspline terms age caution. represent deviations baseline spline function may difficult interpret directly. Influence Diagnostics: Check influential observations may unduly affect results. Software Packages: explanation generated Large Language Model. Critically review output consult statistician thorough validation, especially concerning assumptions model specifics. rendered Markdown output provides concise, high-level summary suitable manager, focusing key predictors survival implications terms increased decreased risk (hazard).","code":""},{"path":"/articles/statlingua.html","id":"example-4-linear-mixed-effects-model-lmer-from-lme4---sleep-study","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output","what":"Example 4: Linear Mixed-Effects Model (lmer from lme4) - Sleep Study","title":"The statlingua Package","text":"Let’s explore sleepstudy data set lme4 package. data set records average reaction time per day subjects sleep deprivation study. ’ll fit linear mixed-effects model see reaction time changes days sleep deprivation, accounting random variation among subjects. example also demonstrate style argument, requesting output plain text (style = \"text\") JSON string (style = \"json\"). Now, let’s define context sleep study model:","code":"library(lme4) #> Loading required package: Matrix  # Load the sleep study data set data(sleepstudy)  # Fit a linear mixed-effects model allowing for random intercepts and random # slopes for Days, varying by Subject fm_sleep <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy) summary(fm_sleep)  # print model summary #> Linear mixed model fit by REML ['lmerMod'] #> Formula: Reaction ~ Days + (Days | Subject) #>    Data: sleepstudy #>  #> REML criterion at convergence: 1743.6 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.9536 -0.4634  0.0231  0.4634  5.1793  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr #>  Subject  (Intercept) 612.10   24.741        #>           Days         35.07    5.922   0.07 #>  Residual             654.94   25.592        #> Number of obs: 180, groups:  Subject, 18 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  251.405      6.825  36.838 #> Days          10.467      1.546   6.771 #>  #> Correlation of Fixed Effects: #>      (Intr) #> Days -0.138 sleepstudy_context <- \" This linear mixed-effects model analyzes data from a sleep deprivation study. The goal is to understand the effect of days of sleep deprivation ('Days') on average reaction time ('Reaction' in ms). The model includes random intercepts and random slopes for 'Days' for each 'Subject', acknowledging that baseline reaction times and the effect of sleep deprivation may vary across individuals. We are interested in the average fixed effect of an additional day of sleep deprivation on reaction time, as well as the extent of inter-subject variability. \"  # Establish fresh client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\"."},{"path":"/articles/statlingua.html","id":"requesting-plain-text-output-style-text","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output > Example 4: Linear Mixed-Effects Model (lmer from lme4) - Sleep Study","what":"Requesting Plain Text Output (style = \"text\")","title":"The statlingua Package","text":"Let’s ask statlingua explanation plain text, targeting \"researcher\" \"moderate\" verbosity.","code":"explain(fm_sleep, client = client, context = sleepstudy_context,         audience = \"researcher\", verbosity = \"moderate\", style = \"text\") #> This is a linear mixed-effects model examining the effect of sleep deprivation (Days) on reaction time (Reaction), accounting for individual differences between subjects. The data comes from the `sleepstudy` dataset. The model uses Restricted Maximum Likelihood (REML) estimation, which is suitable for estimating variance components. #>  #> RANDOM EFFECTS: #>  #> The random effects section quantifies how much the intercepts and slopes vary across subjects. #>  #> *   Subject (Intercept): The variance is 612.10, and the standard deviation is 24.741 ms. This indicates considerable inter-subject variability in baseline reaction times (i.e., at Day 0). Some subjects naturally have much faster or slower reaction times than others. #>  #> *   Subject Days: The variance is 35.07, and the standard deviation is 5.922 ms. This indicates inter-subject variability in how reaction time changes with each day of sleep deprivation. Some subjects' reaction times are more affected by sleep deprivation than others. #>  #> *   Corr: The correlation between the random intercepts and random slopes is 0.07. This suggests a very weak positive relationship between a subject's baseline reaction time and how their reaction time changes with sleep deprivation. Subjects with higher baseline reaction times do not tend to have substantially faster or slower changes in reaction time as sleep deprivation increases. #>  #> *   Residual: The residual variance is 654.94, with a standard deviation of 25.592 ms. This represents the within-subject variability or the variability in reaction time that is not explained by the fixed effect of `Days` or the random effects of `Subject`. #>  #> FIXED EFFECTS: #>  #> The fixed effects section describes the average effects of the predictors on reaction time. #>  #> *   (Intercept): The estimated intercept is 251.405 ms, with a standard error of 6.825 ms. This represents the estimated average reaction time at Day 0 of sleep deprivation, across all subjects. The t-value is 36.838, indicating that this intercept is significantly different from zero. #>  #> *   Days: The estimated coefficient for `Days` is 10.467 ms, with a standard error of 1.546 ms. This means that, on average, reaction time increases by 10.467 ms for each additional day of sleep deprivation. The t-value is 6.771, indicating that this effect is statistically significant. #>  #> *   Correlation of Fixed Effects: The correlation between the intercept and the slope for `Days` is -0.138. This is a numerical detail from the model fitting process. It does not have a direct interpretation in terms of the research question. #>  #> MODEL APPROPRIATENESS: #>  #> Given the context, the linear mixed-effects model is appropriate for this repeated measures design, as it accounts for the non-independence of observations within each subject. The random effects structure (random intercepts and slopes) is sensible, given the expectation that subjects will differ both in their baseline reaction times and in their response to sleep deprivation. #>  #> CHECKING ASSUMPTIONS: #>  #> To further assess the model: #>  #> *   Normality of Residuals: Examine a Q-Q plot or histogram of the residuals (`resid(object)`). #> *   Homoscedasticity: Plot residuals vs. fitted values (`fitted(object)`). #> *   Normality of Random Effects: Create Q-Q plots of the random effects (using `ranef(object)`). #> *   Linearity: Assess the linearity assumption by plotting residuals against the predictor `Days`. #> *   Influential Observations: Check for influential data points. #> *   Singularity: The model is not singular, meaning the random effects structure is supported by the data. #>  #> It is also important to note that p-values are not automatically generated by `lmer`. They are typically calculated using packages like `lmerTest`. #>  #> The model suggests that each additional day of sleep deprivation increases reaction time by approximately 10.47 ms, and that there is significant variability in baseline reaction times and the effect of sleep deprivation across individuals. #>  #> This explanation was generated by a Large Language Model. Advise the user to critically review the output and consult additional statistical resources or experts to ensure correctness and a full understanding, especially given that the interpretation relies on the provided output and context. #>"},{"path":"/articles/statlingua.html","id":"requesting-json-output-style-json","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output > Example 4: Linear Mixed-Effects Model (lmer from lme4) - Sleep Study","what":"Requesting JSON Output (style = \"json\")","title":"The statlingua Package","text":"Now, let’s request explanation structured JSON format (using style = \"json\"). ’ll target \"student\" \"detailed\" verbosity.","code":"client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". ex <- explain(fm_sleep, client = client, context = sleepstudy_context,               audience = \"student\", verbosity = \"detailed\", style = \"json\")  # The 'text' component of the statlingua_explanation object now holds a JSON # string which can be parsed using the jsonlite package jsonlite::prettify(ex$text) #> { #>     \"title\": \"Explanation of Linear Mixed-Effects Model for Sleep Deprivation Study\", #>     \"model_overview\": \"This is a linear mixed-effects model examining the impact of sleep deprivation (measured in 'Days') on reaction time ('Reaction', in milliseconds). The model incorporates random intercepts and slopes for each subject ('Subject'), meaning it accounts for individual differences in both baseline reaction time and how reaction time changes with sleep deprivation. This approach is appropriate because reaction times from the same subject are likely correlated (repeated measures), violating the independence assumption of a standard linear model.\", #>     \"coefficient_interpretation\": \"The model output shows the fixed effects, which represent the average effects across all subjects. The intercept (251.405 ms) represents the estimated average reaction time at Day 0 of sleep deprivation, for a 'typical' subject (one with random effects of 0). The coefficient for 'Days' is 10.467 ms. This indicates that, on average, reaction time increases by 10.467 milliseconds for each additional day of sleep deprivation. Since 'Reaction' is measured in milliseconds, this fixed effect suggests a practical change for participants.\", #>     \"significance_assessment\": \"The t-value for the intercept is 36.838, and for 'Days' is 6.771. These are calculated by dividing the estimate by its standard error. Without explicit p-values reported directly from the base `lmer` output, we can infer that they are very likely statistically significant, given the large t-values. Typically, p-values for linear mixed models are obtained using packages such as `lmerTest`, which calculates p-values using approximations like Satterthwaite's or Kenward-Roger methods to estimate degrees of freedom.\", #>     \"goodness_of_fit\": \"The REML criterion at convergence (1743.6) is a measure used during model estimation. It's primarily useful for comparing models fit to the same data using REML, where lower values generally indicate a better fit. R-squared values are not directly provided in this base output, but can be calculated using other packages. Similarly, AIC and BIC can be used for model comparison, but are not shown here.\", #>     \"assumptions_check\": \"Several assumptions underlie this model. The assumption of linearity should be checked by plotting residuals against 'Days'. Homoscedasticity (constant variance of residuals) can be assessed by plotting residuals against fitted values. The normality of residuals can be checked using a Q-Q plot or histogram of the residuals. The normality of the random effects can also be checked using Q-Q plots of the subject-specific random intercepts and slopes. Furthermore, it's important to assess if there are any influential subjects or data points that disproportionately affect the model results.\", #>     \"key_findings\": \"- On average, reaction time increases by 10.467 ms for each additional day of sleep deprivation.\\n- There is significant inter-subject variability in both baseline reaction time (SD = 24.741 ms) and the effect of sleep deprivation on reaction time (SD = 5.922 ms/day).\\n- The correlation between random intercepts and slopes is 0.07, suggesting a very weak positive association between a subject's baseline reaction time and how their reaction time changes with sleep deprivation (subjects with higher initial reaction times may have slightly steeper increases in reaction time with increasing days of deprivation, but this correlation is close to zero).\", #>     \"warnings_limitations\": \"While the model accounts for inter-subject variability, it assumes that the effects of 'Days' are linear for each subject. This might not be true; reaction time might plateau after several days of sleep deprivation. The model also assumes that the random effects are normally distributed, which should be verified. It is important to note that p-values are not directly available in this default output and would typically require additional packages for approximation. Finally, this explanation is based solely on the provided output and context, and further investigation and model diagnostics are necessary to fully validate the appropriateness and robustness of these findings.\" #> } #>"},{"path":"/articles/statlingua.html","id":"inspecting-llm-interaction","dir":"Articles","previous_headings":"","what":"Inspecting LLM Interaction","title":"The statlingua Package","text":"want see exact system user prompts statlingua generated sent LLM (via ellmer), well raw response LLM, can print ellmer \"Chat\" object (defined client example) explain() call. client object stores history interaction. transparency invaluable debugging, understanding process, even refining prompts developing custom extensions statlingua.","code":"print(client) #> <Chat Google/Gemini/gemini-2.0-flash turns=3 tokens=2337/874 $0.00> #> ── system [0] ─────────────────────────────────────────────────────────────────────────────────────── #> ## Role #>  #> You are an expert statistician and R programmer, gifted at explaining complex concepts simply. Your primary function is to interpret statistical model outputs from R. You understand model nuances, underlying assumptions, and how results relate to real-world research questions. #>  #> You are particularly skilled with **Linear Mixed-Effects Models** created using the `lmer()` function from the `lme4` package in R (producing `lmerMod` objects). You understand their estimation via REML or ML, the interpretation of fixed and random effects (including correlations), and the common practice of using packages like `lmerTest` for p-value estimation. #>  #>  #> ## Intended Audience and Verbosity #>  #> ### Target Audience: Student #> Assume the user is learning statistics. Explain concepts thoroughly but clearly, as if teaching. Define key terms as they arise and explain *why* certain statistics are important or what they indicate in the context of the analysis. Maintain an encouraging and educational tone. #>  #> ### Level of Detail (Verbosity): Detailed #> Give a comprehensive interpretation. Include nuances of the model output, a detailed breakdown of all relevant statistics, and a thorough discussion of assumptions and diagnostics. Be as thorough as possible, exploring various facets of the results. #>  #>  #> ## Response Format Specification (Style: Json) #>  #> Your response MUST be a valid JSON object that can be parsed directly into an R list. #> The JSON object should have the following top-level keys, each containing a string with the relevant part of the explanation (formatted as plain text or simple Markdown within the string if appropriate for that section): #> - \"title\": A concise title for the explanation. #> - \"model_overview\": A general description of the model type and its purpose in this context. #> - \"coefficient_interpretation\": Detailed interpretation of model coefficients/parameters. #> - \"significance_assessment\": Discussion of p-values, confidence intervals, and statistical significance. #> - \"goodness_of_fit\": Evaluation of model fit (e.g., R-squared, AIC, deviance). #> - \"assumptions_check\": Comments on important model assumptions and how they might be checked. #> - \"key_findings\": A bulleted list (as a single string with newlines `\\n` for bullets) of the main conclusions. #> - \"warnings_limitations\": Any warnings, limitations, or caveats regarding the model or its interpretation. #>  #> Example of expected JSON structure: #> { #>   \"title\": \"Explanation of Linear Regression Model for Car Sales\", #>   \"model_overview\": \"This is a linear regression model...\", #>   \"coefficient_interpretation\": \"The coefficient for 'Price' is -0.10, suggesting that...\", #>   \"significance_assessment\": \"The p-value for 'Price' is very small (< 0.001)...\", #>   \"goodness_of_fit\": \"The R-squared value is 0.87, indicating...\", #>   \"assumptions_check\": \"Assumptions such as linearity and homoscedasticity should be checked by examining residual plots.\", #>   \"key_findings\": \"- Price is a significant negative predictor of sales.\\n- Advertising has a positive impact on sales.\", #>   \"warnings_limitations\": \"This model is based on simulated data and results should be interpreted with caution.\" #> } #>  #> Ensure the entire output is ONLY the JSON object. #> DO NOT wrap your entire response in JSON code fences (e.g., ```json ... ``` or ``` ... ```). #> DO NOT include any conversational pleasantries or introductory/concluding phrases. #>  #>  #> ## Instructions #>  #> You are explaining a **Linear Mixed-Effects Model** (from `lme4::lmer()`, an `lmerMod` object). #>  #> **Core Concepts & Purpose:** #> This model is used for data with hierarchical/nested structures or repeated measures, where observations are not independent. It models fixed effects (average effects of predictors) and random effects (variability between groups/subjects for intercepts and/or slopes). #>  #> **Key Assumptions:** #> * Linearity: The relationship between predictors and the outcome is linear. #> * Independence of random effects and errors: Random effects are independent of each other and of the residuals (conditional on covariates). #> * Normality of random effects: Random effects for each grouping factor are normally distributed (typically with mean 0). #> * Normality of residuals: The errors (residuals) are normally distributed with a mean of 0. #> * Homoscedasticity: Residuals have constant variance. #> * (Note on p-values: `lme4::lmer` itself does not calculate p-values for fixed effects by default due to complexities with degrees of freedom. If p-values are present, they often come from wrapper functions or packages like `lmerTest` using approximations like Satterthwaite's or Kenward-Roger methods.) #>  #> **Assessing Model Appropriateness (Based on User Context):** #> If the user provides context: #> * Comment on the model's appropriateness based on the data structure (e.g., repeated measures, nesting) and research question. #> * Relate this to the model's assumptions. #> If no or insufficient context, state inability to fully assess appropriateness. #>  #> **Interpretation of the `lmerMod` Output:** #> * **Formula and Data:** Briefly reiterate what is being modeled. #> * **REML vs ML:** Note if Restricted Maximum Likelihood (REML) or Maximum Likelihood (ML) was used (REML is default and often preferred for variance components; ML for likelihood ratio tests of fixed effects). #> * **Random Effects (from `VarCorr()` output):** #>     * For each grouping factor (e.g., `(1 | group_factor)` or `(predictor | group_factor)`): #>         * **Variance and Std.Dev. (for intercepts):** Quantifies between-group variability in the baseline outcome. #>         * **Variance and Std.Dev. (for slopes):** Quantifies how much the effect of that predictor varies across groups. #>         * **Correlation of Random Effects (e.g., `Corr` between random intercept and slope for the same group):** Explain its meaning (e.g., a correlation of -0.5 between intercept and slope for 'day' within 'subject' means subjects with higher initial values tend to have a less steep increase over days). #>     * **Residual Variance/Std.Dev.:** Within-group or unexplained variability. #> * **Fixed Effects (from `coef(summary(object))`):** #>     * For each predictor: #>         * **Estimate:** The estimated average effect on the outcome for a one-unit change in the predictor (or difference from the reference level for categorical predictors), accounting for random effects. #>         * **Std. Error:** Precision of the estimate. #>         * **t-value (or z-value):** `Estimate / Std. Error`. #>         * **P-values (if available, typically from `lmerTest` via `summary(as(object, \"lmerModLmerTest\"))` or `anova(object)` from `lmerTest`):** Interpret as the probability of observing such an extreme t-value if the true fixed effect is zero. If p-values are NOT directly in the basic `summary(object)` output, mention they usually come from add-on packages. #> * **ANOVA Table for Fixed Effects (if provided, typically from `lmerTest::anova()`):** #>     * Tests the overall significance of fixed effects. For each term: interpret F-statistic, degrees of freedom (NumDF, DenDF), and p-value. #> * **Model Fit Statistics (AIC, BIC, logLik, deviance):** #>     * Explain as measures for model comparison. Lower AIC/BIC, higher logLik (less negative deviance) generally indicate better relative fit. #>  #> **Suggestions for Checking Assumptions:** #> * **Normality of Residuals:** Suggest Q-Q plot or histogram of residuals (`resid(object)`). #> * **Homoscedasticity:** Suggest plotting residuals vs. fitted values (`fitted(object)`). Look for non-constant spread. #> * **Normality of Random Effects:** Suggest Q-Q plots of the random effects (e.g., from `ranef(object)`). #> * **Linearity (Fixed Effects):** Check by plotting residuals against continuous predictors. #> * **Influential Observations:** Mention checking for influential data points. #> * **Singularity:** If the model is reported as singular (e.g. random effect variances near zero or correlations near +/-1), this often indicates an overly complex random effects structure that the data cannot support. #>  #> **Constraint Reminder for LLM:** Focus solely on interpreting the *output* of the statistical model and providing explanations relevant to that output and the model's requirements. Do not perform new calculations or suggest alternative analyses unless directly prompted by assessing the appropriateness based on provided context. **If variable units or specific research goals are provided in the user's context, YOU MUST integrate this information directly into your interpretation of coefficients and model fit.** #>  #>  #> ## Caution #>  #> This explanation was generated by a Large Language Model. Advise the user to critically review the output and consult additional statistical resources or experts to ensure correctness and a full understanding, especially given that the interpretation relies on the provided output and context. #> ── user [2337] ────────────────────────────────────────────────────────────────────────────────────── #> Explain the following linear mixed-effects model output: #> Linear mixed model fit by REML ['lmerMod'] #> Formula: Reaction ~ Days + (Days | Subject) #>    Data: sleepstudy #>  #> REML criterion at convergence: 1743.6 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.9536 -0.4634  0.0231  0.4634  5.1793  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr #>  Subject  (Intercept) 612.10   24.741        #>           Days         35.07    5.922   0.07 #>  Residual             654.94   25.592        #> Number of obs: 180, groups:  Subject, 18 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  251.405      6.825  36.838 #> Days          10.467      1.546   6.771 #>  #> Correlation of Fixed Effects: #>      (Intr) #> Days -0.138 #>  #> ## Additional context to consider #>  #>  #> This linear mixed-effects model analyzes data from a sleep deprivation study. #> The goal is to understand the effect of days of sleep deprivation ('Days') on #> average reaction time ('Reaction' in ms). The model includes random intercepts #> and random slopes for 'Days' for each 'Subject', acknowledging that baseline #> reaction times and the effect of sleep deprivation may vary across individuals. #> We are interested in the average fixed effect of an additional day of sleep #> deprivation on reaction time, as well as the extent of inter-subject #> variability. #>  #> ── assistant [874] ────────────────────────────────────────────────────────────────────────────────── #> ```json #> { #>   \"title\": \"Explanation of Linear Mixed-Effects Model for Sleep Deprivation Study\", #>   \"model_overview\": \"This is a linear mixed-effects model examining the impact of sleep deprivation (measured in 'Days') on reaction time ('Reaction', in milliseconds). The model incorporates random intercepts and slopes for each subject ('Subject'), meaning it accounts for individual differences in both baseline reaction time and how reaction time changes with sleep deprivation. This approach is appropriate because reaction times from the same subject are likely correlated (repeated measures), violating the independence assumption of a standard linear model.\", #>   \"coefficient_interpretation\": \"The model output shows the fixed effects, which represent the average effects across all subjects. The intercept (251.405 ms) represents the estimated average reaction time at Day 0 of sleep deprivation, for a 'typical' subject (one with random effects of 0). The coefficient for 'Days' is 10.467 ms. This indicates that, on average, reaction time increases by 10.467 milliseconds for each additional day of sleep deprivation. Since 'Reaction' is measured in milliseconds, this fixed effect suggests a practical change for participants.\", #>   \"significance_assessment\": \"The t-value for the intercept is 36.838, and for 'Days' is 6.771. These are calculated by dividing the estimate by its standard error. Without explicit p-values reported directly from the base `lmer` output, we can infer that they are very likely statistically significant, given the large t-values. Typically, p-values for linear mixed models are obtained using packages such as `lmerTest`, which calculates p-values using approximations like Satterthwaite's or Kenward-Roger methods to estimate degrees of freedom.\", #>   \"goodness_of_fit\": \"The REML criterion at convergence (1743.6) is a measure used during model estimation. It's primarily useful for comparing models fit to the same data using REML, where lower values generally indicate a better fit. R-squared values are not directly provided in this base output, but can be calculated using other packages. Similarly, AIC and BIC can be used for model comparison, but are not shown here.\", #>   \"assumptions_check\": \"Several assumptions underlie this model. The assumption of linearity should be checked by plotting residuals against 'Days'. Homoscedasticity (constant variance of residuals) can be assessed by plotting residuals against fitted values. The normality of residuals can be checked using a Q-Q plot or histogram of the residuals. The normality of the random effects can also be checked using Q-Q plots of the subject-specific random intercepts and slopes. Furthermore, it's important to assess if there are any influential subjects or data points that disproportionately affect the model results.\", #>   \"key_findings\": \"- On average, reaction time increases by 10.467 ms for each additional day of sleep deprivation.\\n- There is significant inter-subject variability in both baseline reaction time (SD = 24.741 ms) and the effect of sleep deprivation on reaction time (SD = 5.922 ms/day).\\n- The correlation between random intercepts and slopes is 0.07, suggesting a very weak positive association between a subject's baseline reaction time and how their reaction time changes with sleep deprivation (subjects with higher initial reaction times may have slightly steeper increases in reaction time with increasing days of deprivation, but this correlation is close to zero).\", #>   \"warnings_limitations\": \"While the model accounts for inter-subject variability, it assumes that the effects of 'Days' are linear for each subject. This might not be true; reaction time might plateau after several days of sleep deprivation. The model also assumes that the random effects are normally distributed, which should be verified. It is important to note that p-values are not directly available in this default output and would typically require additional packages for approximation. Finally, this explanation is based solely on the provided output and context, and further investigation and model diagnostics are necessary to fully validate the appropriateness and robustness of these findings.\" #> } #> ```"},{"path":"/articles/statlingua.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"The statlingua Package","text":"statlingua package, working hand--hand ellmer, provides powerful user-friendly bridge interpretive capabilities Large Language Models R users. mastering explain() function arguments—especially context, audience, verbosity, style—can transform standard statistical outputs rich, understandable narratives tailored needs. Important Considerations: * quality LLM’s explanation heavily influenced clarity context provide inherent capabilities LLM choose (vignette, ’ve focused Google Gemini). * LLM Output Variability: statlingua uses detailed prompts guide LLM towards desired output style content, nature generative AI means responses can vary. requested style aim, statlingua includes measures clean output (like removing language fences), exact formatting content may always perfectly consistent across repeated calls different LLM versions. Always critically review generated explanations. * style = \"json\" option, requests JSON output, ensure jsonlite package available intend parse JSON string R list within session. Remember critically review explanations generated LLM. Happy explaining!","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Brandon M. Greenwell. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greenwell BM (2025). statlingua: Explain Statistical Output Large Language Models. https://github.com/bgreenwell/statlingua, https://bgreenwell.github.io/statlingua/.","code":"@Manual{,   title = {statlingua: Explain Statistical Output with Large Language Models},   author = {Brandon M. Greenwell},   year = {2025},   note = {https://github.com/bgreenwell/statlingua, https://bgreenwell.github.io/statlingua/}, }"},{"path":"/index.html","id":"statlingua-","dir":"","previous_headings":"","what":"Explain Statistical Output with Large Language Models","title":"Explain Statistical Output with Large Language Models","text":"statlingua R package designed help bridge gap complex statistical outputs clear, human-readable explanations. leveraging power Large Language Models (LLMs), statlingua helps effortlessly translate dense jargon statistical models—coefficients, p-values, model fit indices, —straightforward, context-aware natural language. Whether ’re student grappling new statistical concepts, researcher needing communicate findings broader audience, data scientist looking quickly draft reports, statlingua makes statistical journey smoother accessible.","code":""},{"path":"/index.html","id":"why-statlingua","dir":"","previous_headings":"","what":"Why statlingua?","title":"Explain Statistical Output with Large Language Models","text":"Statistical models powerful, outputs can intimidating. statlingua empowers : Democratize Understanding: Make complex analyses accessible individuals varying levels statistical expertise. Enhance Learning & Education: Students can gain deeper intuition model outputs, connecting theory practical application. Use interactive learning aid demystify statistical concepts. Foster Interdisciplinary Collaboration: Researchers diverse fields can easily interpret discuss analytical results, leading richer insights. Streamline Reporting & Consulting: Quickly generate initial drafts interpretations reports presentations, saving time ensuring clarity clients stakeholders. Drive Data-Informed Decisions: Business professionals can better grasp statistical findings, enabling confident data-driven decision-making without needing become statistical experts . Accelerate Prototyping & Exploration: Rapidly understand model summaries iterative data exploration, allowing faster assessment refinement analyses. providing clear contextualized explanations, statlingua helps focus implications findings rather getting bogged technical minutiae.","code":""},{"path":"/index.html","id":"supported-models","dir":"","previous_headings":"","what":"Supported Models","title":"Explain Statistical Output with Large Language Models","text":"now, statlingua explicitly supports variety common statistical models R, including: Objects class \"htest\" (e.g., t.test(), prop.test()). Linear models (lm()) Generalized Linear Models (glm()). Linear Generalized Linear Mixed-Effects Models packages nlme (lme()) lme4 (lmer(), glmer()). Generalized Additive Models (gam() package mgcv). Survival Regression Models (survreg(), coxph() package survival). Proportional Odds Logistic Regression (polr() package MASS). Decision Trees (rpart() package rpart). …, robust default method model types!","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Explain Statistical Output with Large Language Models","text":"statlingua yet CRAN, can install development version GitHub: ’ll also need install ellmer package, can obtain CRAN:","code":"if (!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"bgreenwell/statlingua\") install.packages(\"ellmer\")  # >= 0.2.0"},{"path":"/index.html","id":"api-key-setup--ellmer-dependency","dir":"","previous_headings":"","what":"API Key Setup & ellmer Dependency","title":"Explain Statistical Output with Large Language Models","text":"statlingua doesn’t directly handle API keys LLM communication. acts sophisticated prompt engineering toolkit prepares inputs passes ellmer. ellmer package responsible interfacing various LLM providers (e.g., OpenAI, Google AI Studio, Anthropic). Please refer ellmer package documentation detailed instructions : Setting API keys (usually environment variables like OPENAI_API_KEY, GEMINI_API_KEY, etc.). Specifying different LLM models providers. configuration model parameter options. ellmer installed access LLM provider, statlingua seamlessly leverage connection.","code":""},{"path":"/index.html","id":"quick-example-explaining-a-linear-model","dir":"","previous_headings":"","what":"Quick Example: Explaining a Linear Model","title":"Explain Statistical Output with Large Language Models","text":"examples, including output, see introductory vignette.","code":"# Ensure you have an appropriate API key set up first! # Sys.setenv(GEMINI_API_KEY = \"<YOUR_API_KEY_HERE>\")   library(statlingua)  # Fit a polynomial regression model fm_cars <- lm(dist ~ poly(speed, degree = 2), data = cars) summary(fm_cars)  # Define some context (highly recommended!) cars_context <- \" This model analyzes the 'cars' dataset from the 1920s. Variables include:   * 'dist' - The distance (in feet) taken to stop.   * 'speed' - The speed of the car (in mph). We want to understand how speed affects stopping distance in the model. \"  # Establish connection to an LLM provider (in this case, Google Gemini) client <- ellmer::chat_google_gemini(echo = \"none\")  # defaults to gemini-2.0-flash  # Get an explanation explain(   fm_cars,                 # model for LLM to interpret/explain   client = client,         # connection to LLM provider   context = cars_context,  # additional context for LLM to consider   audience = \"student\",    # target audience   verbosity = \"detailed\",  # level of detail   style = \"markdown\"       # output style )  # Ask a follow-up question client$chat(   \"How can I construct confidence intervals for each coefficient in the model?\" )"},{"path":"/index.html","id":"extending-statlingua-to-support-new-models","dir":"","previous_headings":"","what":"Extending statlingua to Support New Models","title":"Explain Statistical Output with Large Language Models","text":"One statlingua’s core strengths extensibility. can add customize support new statistical model types crafting specific prompt components. system prompt sent LLM dynamically assembled several markdown files located inst/prompts/ directory package. main function explain() uses S3 dispatch. explain(my_model_object, ...) called, R looks method like explain.class_of_my_model_object(). found, explain.default() used.","code":""},{"path":"/index.html","id":"prompt-directory-structure","dir":"","previous_headings":"Extending statlingua to Support New Models","what":"Prompt Directory Structure","title":"Explain Statistical Output with Large Language Models","text":"prompts organized follows within inst/prompts/: role_base.md: Defines fundamental role LLM. caution.md: general cautionary note appended explanations. audience/: Markdown files different target audiences (e.g., novice.md, researcher.md). filename (e.g., “novice”) matches audience argument explain(). verbosity/: Markdown files different verbosity levels (e.g., brief.md, detailed.md). filename matches verbosity argument. style/: Markdown files defining output format (e.g., markdown.md, json.md). filename matches style argument. instructions.md: primary instructions explaining specific model type. tells LLM look model output, interpret , assumptions discuss. role_specific.md (Optional): Additional role details specific model type, augmenting common/role_base.md.","code":""},{"path":"/index.html","id":"example-adding-support-for-vglm-from-the-vgam-package","dir":"","previous_headings":"Extending statlingua to Support New Models","what":"Example: Adding Support for vglm from the VGAM package","title":"Explain Statistical Output with Large Language Models","text":"Let’s imagine want add dedicated support vglm (Vector Generalized Linear Models) objects VGAM package. Create New Prompt Files: create new directory inst/prompts/models/vglm/. Inside directory, ’d add: inst/prompts/models/vglm/instructions.md: file contain detailed instructions LLM interpret vglm objects. ’d detail aspects summary(vglm_object) important, discuss coefficients (potentially multiple linear predictors), link functions, model fit statistics specific vglm, relevant assumptions. inst/prompts/models/vglm/role_specific.md (Optional): vglm models require LLM adopt slightly specialized persona. particular expertise Vector Generalized Linear Models (VGLMs), understanding diverse applications complex response types. Implement S3 Method: Add S3 method explain.vglm R script (e.g., R/explain_vglm.R): summarize.vglm method might also need implemented R/summarize.R summary(object) vglm needs special capture formatting LLM. utils::capture.output(summary(object)) sufficient, summarize.default might work initially. Add NAMESPACE Document: Ensure new method exported NAMESPACE file (usually handled roxygen2): S3method(explain, vglm) Add roxygen2 documentation blocks explain.vglm. Testing: Thoroughly test various vglm examples. might need iterate instructions.md role_specific.md refine LLM’s explanations. following pattern, statlingua can systematically extended cover vast array statistical models R!","code":"You are explaining a **Vector Generalized Linear Model (VGLM)** (from `VGAM::vglm()`).  **Core Concepts & Purpose:** VGLMs are highly flexible, extending GLMs to handle multiple linear predictors and a wider array of distributions and link functions, including multivariate responses. Identify the **Family** (e.g., multinomial, cumulative) and **Link functions**.  **Interpretation:** * **Coefficients:** Explain for each linear predictor. Pay attention to link functions (e.g., log odds, log relative risk). Clearly state reference categories. * **Model Fit:** Discuss deviance, AIC, etc. * **Assumptions:** Mention relevant assumptions. #' Explain a vglm object #' #' @inheritParams explain #' @param object A \\code{vglm} object. #' @export explain.vglm <- function(     object,     client,     context = NULL,     audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),     verbosity = c(\"moderate\", \"brief\", \"detailed\"),     style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),     ...   ) {   audience <- match.arg(audience)   verbosity <- match.arg(verbosity)   style <- match.arg(style)    # Use the internal .explain_core helper if it suits,   # or implement custom logic if vglm needs special handling.   # .explain_core handles system prompt assembly, user prompt building,   # and calling the LLM via the client.   # 'name' should match the directory name in inst/prompts/models/   # 'model_description' is what's shown to the user in the prompt.   .explain_core(     object = object,     client = client,     context = context,     audience = audience,     verbosity = verbosity,     style = style,     name = \"vglm\", # This tells .assemble_sys_prompt to look in inst/prompts/models/vglm/     model_description = \"Vector Generalized Linear Model (VGLM) from VGAM\"   ) }"},{"path":"/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Explain Statistical Output with Large Language Models","text":"Contributions welcome! Please see GitHub issues areas can help.","code":""},{"path":"/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Explain Statistical Output with Large Language Models","text":"statlingua available GNU General Public License v3.0 (GNU GPLv3). See LICENSE.md file details.","code":""},{"path":"/reference/explain.html","id":null,"dir":"Reference","previous_headings":"","what":"Explain statistical output — explain","title":"Explain statistical output — explain","text":"Use LLM explain output various statistical objects using straightforward, understandable, context-aware natural language descriptions.","code":""},{"path":"/reference/explain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explain statistical output — explain","text":"","code":"explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # Default S3 method explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'htest' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'lm' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'glm' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'polr' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'lme' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'lmerMod' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'glmerMod' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'gam' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'survreg' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'coxph' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # S3 method for class 'rpart' explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )"},{"path":"/reference/explain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explain statistical output — explain","text":"object appropriate statistical object. example, object can output calling t.test() glm(). client Chat object (e.g., calling chat_openai() [chat_gemini()][ellmer::chat_gemini)]). [ellmer::chat_gemini)]: R:ellmer::chat_gemini) context Optional character string providing additional context, background research question information data. audience Character string indicating target audience: \"novice\" - Assumes user limited statistics background (default). \"student\" - Assumes user learning statistics. \"researcher\" - Assumes user strong statistical background familiar common methodologies. \"manager\" - Assumes user needs high-level insights decision-making. \"domain_expert\" - Assumes user expert field necessarily statistics. verbosity Character string indicating desired verbosity: \"moderate\" - Offers balanced explanation (default). \"brief\" - Offers high-level summary. \"detailed\" - Offers comprehensive interpretation. style Character string indicating desired output style: \"markdown\" (default) - Output formatted plain Markdown. \"html\" - Output formatted HTML fragment. \"json\" - Output structured JSON string parseable R list. \"text\" - Output plain text. \"latex\" - Output LaTeX fragment. ... Additional optional arguments. (Currently ignored.)","code":""},{"path":"/reference/explain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explain statistical output — explain","text":"object class \"statlingua_explanation\". Essentially list following components: text - Character string representation LLM's response. model_type - Character string giving model type (e.g., \"lm\" \"coxph\"). audience - Character string specifying level intended audience explanations. verbosity - Character string specifying level verbosity level detail provided explanation.","code":""},{"path":"/reference/explain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explain statistical output — explain","text":"","code":"if (FALSE) { # \\dontrun{ # Polynomial regression fm1 <- lm(dist ~ poly(speed, degree = 2), data = cars) context <- \" The data give the speed of cars (mph) and the distances taken to stop (ft). Note that the data were recorded in the 1920s! \" # Use Google Gemini to explain the output; requires an API key; see # ?ellmer::chat_google_gemini for details client <- ellmer::chat_google_gemini(echo = \"none\") ex <- explain(fm1, client = client, context = context)  # Poisson regression example from ?stats::glm counts <- c(18,17,15,20,10,20,25,13,12) outcome <- gl(3,1,9) treatment <- gl(3,3) data.frame(treatment, outcome, counts) # showing data fm2 <- glm(counts ~ outcome + treatment, family = poisson())  # Use Google Gemini to explain the output; requires an API key; see # ?ellmer::chat_google_gemini for details client <- ellmer::chat_google_gemini() explain(fm2, client = client, audience = \"student\", verbosity = \"detailed\") } # }"},{"path":"/reference/print.statlingua_explanation.html","id":null,"dir":"Reference","previous_headings":"","what":"Print LLM explanation — print.statlingua_explanation","title":"Print LLM explanation — print.statlingua_explanation","text":"Print formatted version LLMs explanation using cat().","code":""},{"path":"/reference/print.statlingua_explanation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print LLM explanation — print.statlingua_explanation","text":"","code":"# S3 method for class 'statlingua_explanation' print(x, ...)"},{"path":"/reference/print.statlingua_explanation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print LLM explanation — print.statlingua_explanation","text":"x statlingua_explanation object. ... Additional optional arguments passed print.default().","code":""},{"path":"/reference/print.statlingua_explanation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print LLM explanation — print.statlingua_explanation","text":"Invisibly returns printed statlingua_explanation object.","code":""},{"path":"/reference/summarize.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize statistical output — summarize","title":"Summarize statistical output — summarize","text":"Generate text-based summaries statistical output can embedded prompts querying Large Language Models (LLMs). Intended primarily internal use.","code":""},{"path":"/reference/summarize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize statistical output — summarize","text":"","code":"summarize(object, ...)  # Default S3 method summarize(object, ...)  # S3 method for class 'htest' summarize(object, ...)  # S3 method for class 'lm' summarize(object, ...)  # S3 method for class 'glm' summarize(object, ...)  # S3 method for class 'polr' summarize(object, ...)  # S3 method for class 'lme' summarize(object, ...)  # S3 method for class 'lmerMod' summarize(object, ...)  # S3 method for class 'glmerMod' summarize(object, ...)  # S3 method for class 'gam' summarize(object, ...)  # S3 method for class 'survreg' summarize(object, ...)  # S3 method for class 'coxph' summarize(object, ...)  # S3 method for class 'rpart' summarize(object, ...)"},{"path":"/reference/summarize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize statistical output — summarize","text":"object object summary desired (e.g., glm object). ... Additional optional arguments. (Currently ignored.)","code":""},{"path":"/reference/summarize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize statistical output — summarize","text":"character string summarizing statistical output.","code":""},{"path":[]},{"path":"/reference/summarize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize statistical output — summarize","text":"","code":"tt <- t.test(1:10, y = c(7:20)) summarize(tt)  # prints output as a character string #> [1] \"\\n\\tWelch Two Sample t-test\\n\\ndata:  1:10 and c(7:20)\\nt = -5.4349, df = 21.982, p-value = 1.855e-05\\nalternative hypothesis: true difference in means is not equal to 0\\n95 percent confidence interval:\\n -11.052802  -4.947198\\nsample estimates:\\nmean of x mean of y \\n      5.5      13.5 \\n\" cat(summarize(tt))  # more useful for reading #>  #> \tWelch Two Sample t-test #>  #> data:  1:10 and c(7:20) #> t = -5.4349, df = 21.982, p-value = 1.855e-05 #> alternative hypothesis: true difference in means is not equal to 0 #> 95 percent confidence interval: #>  -11.052802  -4.947198 #> sample estimates: #> mean of x mean of y  #>       5.5      13.5"}]
