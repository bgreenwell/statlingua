[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"/TODO.html","id":"todo","dir":"","previous_headings":"","what":"TODO","title":"NA","text":"Systematically add support popular statistical modeling packages (e.g., brms, rstanarm, tidymodels workflows, specific time series models, glmmTMB). Allow users request explanations specific parts summary output (e.g., “explain just random effects table,” “explain ANOVA table fixed effects,” “explain specific coefficients”). Allow users supply complete prompt templates modify existing ones without altering package installation (e.g., via argument like prompt_template_dir functions export/view/load modified default prompts). Implement robust validation LLM’s output valid JSON style = \"json\" used. Define structured predictable JSON schema model type make JSON output reliable programmatic extraction information. Consider option return explanation structured R list directly, just JSON string. Provide helper functions examples easily integrating statlingua explanations R Markdown, Quarto, reporting tools. Ensure style = \"latex\" style = \"html\" fragments clean directly usable. Explore creating helper functions like to_quarto_block() to_rmd_chunk() wrap explanations appropriately.","code":""},{"path":"/articles/examples.html","id":"explaining-the-output-from-a-two-sample-t-test","dir":"Articles","previous_headings":"","what":"Explaining the output from a two-sample t-test","title":"examples","text":"following example taken tutorial paired t-tests. use independent two-sample t-test (inappropriately) analyze paired data. data: exam_scoresexam1scoreandexamscoresexam_1_score exam_scoresexam_2_score t = -0.33602, df = 27.307, p-value = 0.7394 alternative hypothesis: true difference means equal 0 95 percent confidence interval: -9.322782 6.697782 sample estimates: mean x mean y 78.1250 79.4375 ’s explanation Welch Two Sample t-test output, given context ’ve provided.","code":"library(statlingua)  # Define additional context to pass to `explain()`; this should include  # any additional background information about the data and research question. context <- \" An instructor wants to use two exams in her classes next year. This year, she gives both exams to the students. She wants to know if the exams are equally difficult and wants to check this by comparing the two sets of scores. Here is the data:   student exam_1_score exam_2_score      Bob           63           69     Nina           65           65      Tim           56           62     Kate          100           91   Alonzo           88           78     Jose           83           87   Nikhil           77           79    Julia           92           88    Tohru           90           85  Michael           84           92     Jean           68           69    Indra           74           81    Susan           87           84    Allen           64           75     Paul           71           84   Edwina           88           82 \"  # Create the data set exam_scores <- tibble::tribble(   ~student,  ~exam_1_score, ~exam_2_score,   \"Bob\",     63,            69,   \"Nina\",    65,            65,   \"Tim\",     56,            62,   \"Kate\",    100,           91,   \"Alonzo\",  88,            78,   \"Jose\",    83,            87,   \"Nikhil\",  77,            79,   \"Julia\",   92,            88,   \"Tohru\",   90,            85,   \"Michael\", 84,            92,   \"Jean\",    68,            69,   \"Indra\",   74,            81,   \"Susan\",   87,            84,   \"Allen\",   64,            75,   \"Paul\",    71,            84,   \"Edwina\",  88,            82 )  # Run a two-sample t-test (tt <- t.test(exam_scores$exam_1_score, y = exam_scores$exam_2_score)) Welch Two Sample t-test # Inititalize client client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". ex <- explain(tt, client = client, context = context) cat(ex)"},{"path":"/articles/examples.html","id":"summary-of-the-statistical-test","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"1. Summary of the Statistical Test","title":"examples","text":"Name test: Welch Two Sample t-test (also known Independent Samples t-test unequal variances). Purpose: test used compare means two independent groups. case, instructor using compare average score exam 1 average score exam 2. goal determine ’s statistically significant difference mean scores two exams. two samples (exam scores exam 1 exam 2) independent. data group approximately normally distributed. variances two groups assumed equal (’s key difference Welch’s t-test Student’s t-test).","code":""},{"path":"/articles/examples.html","id":"appropriateness-of-the-statistical-test","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"2. Appropriateness of the Statistical Test","title":"examples","text":"Given instructor’s research question data available, Welch Two Sample t-test appears reasonable choice. instructor wants compare difficulty two exams comparing scores, t-test designed comparing means. Welch’s version used don’t know variance exam scores two exams. test inappropriate assumptions met.","code":""},{"path":"/articles/examples.html","id":"suggestions-for-checking-assumptions-of-the-statistical-test","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"3. Suggestions for Checking Assumptions of the Statistical Test","title":"examples","text":"drawing firm conclusions, ’s crucial check assumptions t-test. ’s : Independence: assumption relies study design. Since student took exams, treating set exam scores separate samples, assumption might met. paired t-test might appropriate , scores student different exams independent. Histograms: Create histograms exam_1_score exam_2_score separately. Look bell-shaped distributions roughly symmetrical. Large deviations normality (skewness, multiple peaks) concern. Q-Q Plots: Create quantile-quantile plots (Q-Q plots) sets exam scores. data normally distributed, points Q-Q plot fall approximately along straight line. Deviations line indicate departures normality. Use function qqnorm() R check normality graphically. Shapiro-Wilk Test: can use Shapiro-Wilk test (shapiro.test() R) formally test normality. However, cautious interpreting results, especially small sample sizes, test can overly sensitive. Focus graphical methods first. Side--side boxplots: Compare spread (IQR, interquartile range) two boxplots. one boxplot much wider , indicates larger variance. Levene’s test: leveneTest() car package R provides formal test equality variances.","code":""},{"path":"/articles/examples.html","id":"interpretation-of-the-output","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"4. Interpretation of the Output","title":"examples","text":"Let’s break output: t = -0.33602: calculated t-statistic. represents difference sample means relative variability within samples. value 0, stronger evidence null hypothesis. df = 27.307: degrees freedom. Welch’s t-test assume equal variances, degrees freedom calculated complex way Student’s t-test. degrees freedom used determine p-value. p-value = 0.7394: probability observing t-statistic extreme , extreme , -0.33602 truly difference mean exam scores (.e., null hypothesis true). words, even two exams equally difficult, ’s 73.94% chance ’d see difference sample means big (bigger ) one observed sample. alternative hypothesis: true difference means equal 0: states alternative hypothesis tested: population means two exam scores different. two-sided test, meaning ’s looking differences either direction (exam 1 harder exam 2 harder). 95 percent confidence interval: -9.322782  6.697782: provides range plausible values true difference population means (mean exam 1 minus mean exam 2). 95% confident true difference means lies -9.32 6.70. interval includes 0, suggests difference zero (.e., difference means) plausible value. units exam score points. mean x = 78.1250: sample mean exam 1 scores. mean y = 79.4375: sample mean exam 2 scores.","code":""},{"path":"/articles/examples.html","id":"overall-conclusion","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"5. Overall Conclusion","title":"examples","text":"Using significance level α=0.05\\alpha = 0.05, since p-value (0.7394) greater α\\alpha, fail reject null hypothesis. insufficient evidence conclude statistically significant difference mean scores two exams. Based analysis, instructor enough evidence say one exam significantly harder .","code":""},{"path":"/articles/examples.html","id":"caution","dir":"Articles","previous_headings":"Explaining the output from a two-sample t-test","what":"6. Caution","title":"examples","text":"explanation generated Large Language Model. Critically review output consult additional statistical resources experts ensure correctness full understanding. Remember check assumptions test ensure validity conclusions. Furthermore, given scores student obtained tests, assumption independence may hold. paired t-test may appropriate.","code":""},{"path":"/articles/statlingua.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"The statlingua Package","text":"Statistical models indispensable tools extracting insights data, yet outputs can often cryptic laden technical jargon. Deciphering coefficients, p-values, confidence intervals, various model fit statistics typically requires solid statistical background. can create barrier communicating findings wider audience even still developing statistical acumen. statlingua R package change ! masterfully leverages power Large Language Models (LLMs) translate complex statistical model outputs clear, understandable, context-aware natural language. simply feeding R statistical model objects statlingua, can generate human-readable interpretations, making statistical understanding accessible everyone, regardless technical expertise. ’s important note statlingua doesn’t directly call LLM APIs. Instead, serves sophisticated prompt engineering toolkit. meticulously prepares necessary inputs (model summary contextual information) passes ellmer package, handles actual communication LLM. primary workhorse function ’ll use statlingua explain(). vignette guide understanding using statlingua effectively.","code":""},{"path":"/articles/statlingua.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"The statlingua Package","text":"diving , please ensure following: statlingua package installed GitHub (yet available CRAN): ellmer package installed. statlingua relies LLM communication: Access LLM provider (e.g., OpenAI, Google Gemini, Anthropic) corresponding API key. ’ll need configure API key according ellmer package’s documentation. usually involves setting environment variables like OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY. Note ellmer supports numerous LLM providers, vignette specifically use Google Gemini models via ellmer::chat_google_gemini(); find Google Gemini particularly well-suited explaining statistical output offer generous free tier. ’ll need configure API key according ellmer package’s documentation. typically involves setting GEMINI_API_KEY environment variable R session .Renviron file (e.g., Sys.setenv(GEMINI_API_KEY = \"YOUR_API_KEY_HERE\")). examples vignette, ’ll also need following packages: ISLR2, MASS, survival.","code":"if (!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"bgreenwell/statlingua\") install.packages(\"ellmer\") install.packages(c(\"ISLR2\", \"jsonlite\", \"lme4\", \"MASS\", \"survival\"))"},{"path":"/articles/statlingua.html","id":"how-statlingua-works-the-explain-function-and-ellmer","dir":"Articles","previous_headings":"","what":"How statlingua Works: The explain() Function and ellmer","title":"The statlingua Package","text":"primary function ’ll use ****statlingua**** explain(). S3 generic function, meaning behavior adapts class R statistical object provide (e.g., \"lm\" object, \"glm\" object, \"lmerMod\" object, etc.). process explain() follows generate interpretation involves several key steps: Input & Initial Argument Resolution: call explain() statistical object, ellmer client, optionally context, audience, verbosity, new style argument. explain() generic function first resolves audience, verbosity, style specific chosen values (e.g., audience = \"novice\", style = \"markdown\") using match.arg(). resolved values passed appropriate S3 method class object. Model Summary Extraction: Internally, explain() (typically via .explain_core() helper function directly explain.default()) uses summarize() function capture text-based summary statistical object. captured text (e.g., similar summary(object) produce) forms core statistical information LLM interpret. System Prompt Assembly (via .assemble_sys_prompt()): statlingua constructs detailed instructions LLM. internal .assemble_sys_prompt() function pieces together several components, read .md files stored within package’s inst/prompts/ directory. final system prompt typically includes following sections, ordered guide LLM effectively: base role description read inst/prompts/common/role_base.md. available, model-specific role details appended inst/prompts/models/<model_name>/role_specific.md (<model_name> corresponds class object, like “lm” “lmerMod”). file doesn’t exist specific model, part omitted. Instructions tailored specified audience (e.g., \"novice\", \"researcher\") read inst/prompts/audience/<audience_value>.md (e.g., inst/prompts/audience/novice.md). Instructions defining verbosity level (e.g., \"brief\", \"detailed\") read inst/prompts/verbosity/<verbosity_value>.md (e.g., inst/prompts/verbosity/detailed.md). crucial part determined style argument. Instructions desired output format (e.g., \"markdown\", \"html\", \"json\", \"text\", \"latex\") read inst/prompts/style/<style_value>.md (e.g., inst/prompts/style/markdown.md). tells LLM structure entire response. Detailed instructions aspects statistical model explain read inst/prompts/models/<model_name>/instructions.md (e.g., \"lm\" object, read inst/prompts/models/lm/instructions.md). model-specific instructions aren’t found, defaults inst/prompts/models/default/instructions.md. general caution message appended inst/prompts/common/caution.md. components assembled single, comprehensive system prompt guides LLM’s behavior, tone, content focus, output format. User Prompt Construction (via .build_usr_prompt()): “user prompt” (actual query containing data interpreted) constructed combining: leading phrase indicating type model (e.g., “Explain following linear regression model output:”). captured model output_summary step 2. additional context string provided user via context argument. LLM Interaction via ellmer: assembled sys_prompt set ellmer client object. , constructed usr_prompt sent LLM using client$chat(usr_prompt). ellmer handles actual API communication. Output Post-processing (via .remove_fences()): returning explanation, ****statlingua**** calls internal utility, .remove_fences(), clean LLM’s raw output. function attempts remove common “language fence” wrappers (like markdown ... json ...) LLMs sometimes add around responses. Output Packaging: cleaned explanation string LLM packaged statlingua_explanation object. object’s text component holds explanation string specified style. also includes metadata like model_type, audience, verbosity, style used. statlingua_explanation object default print method uses cat() easy viewing console. comprehensive modular approach prompt engineering allows statlingua provide tailored well-formatted explanations variety statistical models user needs.","code":""},{"path":"/articles/statlingua.html","id":"understanding-explains-arguments","dir":"Articles","previous_headings":"How statlingua Works: The explain() Function and ellmer","what":"Understanding explain()’s Arguments","title":"The statlingua Package","text":"explain() function flexible, several arguments fine-tune behavior: object: primary input – R statistical object (e.g., \"lm\" model, \"glm\" model, output t.test(), coxph(), etc.). client: Essential. ellmer client object (e.g., created ellmer::chat_google_gemini()). statlingua uses communicate LLM. must initialize configure client API key beforehand. context (Optional Highly Recommended): character string providing background information data, research questions, variable definitions, units, study design, etc. Default NULL. audience (Optional): Specifies target audience explanation. Options include: \"novice\" (default), \"student\", \"researcher\", \"manager\", \"domain_expert\". verbosity (Optional): Controls level detail. Options : \"moderate\" (default), \"brief\", \"detailed\". \"markdown\": Output formatted Markdown. \"html\": Output formatted HTML fragment. \"json\": Output structured JSON string parseable R list (see example parsing). \"text\": Output plain text. \"latex\": Output LaTeX fragment. ... (Optional): Additional optional arguments (currently ignored statlingua’s explain methods).","code":""},{"path":"/articles/statlingua.html","id":"the-power-of-context-why-it-matters","dir":"Articles","previous_headings":"","what":"The Power of context: Why It Matters","title":"The statlingua Package","text":"just pass model object explain() get basic interpretation. However, unlock truly insightful actionable explanations, providing context paramount. LLMs incredibly powerful, don’t inherently know nuances specific research. don’t know “VarX” really means data set, units, specific hypothesis ’re testing, population ’re studying unless tell . context argument channel provide vital background. makes effective context? Research Objective: question(s) trying answer? (e.g., “investigating factors affecting Gentoo penguin bill length understand dietary adaptations.”) variables represent? specific. (e.g., “bill_length_mm length penguin’s bill millimeters.”) units? (e.g., “Flipper length millimeters, body mass grams.”) known data limitations special characteristics? (e.g., “Data collected three islands Palmer Archipelago.”) Study Design: data collected? (e.g., “Observational data field study.”) Target Audience Nuances (Implicitly): audience argument handles main targeting, mentioning specific interpretation needs context can refine LLM’s output (e.g., “Explain practical significance findings wildlife conservation efforts.”). supplying details, empower LLM : Interpret coefficients true, domain-specific meaning. Relate findings directly research goals. Offer relevant advice model assumptions limitations. Generate explanations less generic, targeted, ultimately far useful. Think context difference asking generic statistician “mean?” versus asking statistician deeply understands research area, data, objectives. latter always provide valuable interpretation.","code":""},{"path":"/articles/statlingua.html","id":"some-examples-in-action","dir":"Articles","previous_headings":"","what":"Some Examples in Action!","title":"The statlingua Package","text":"Let’s see statlingua shine practical examples. Important Note API Keys: following code chunks call explain() set eval = FALSE default vignette. require active API key configured ellmer. run examples : Ensure API key (e.g., GOOGLE_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY) set environment variable ellmer can access. Change R chunk option eval = FALSE eval = TRUE chunks wish run. may need adjust ellmer client initialization (e.g., ellmer::chat_openai()) match chosen LLM provider. examples vignette","code":""},{"path":"/articles/statlingua.html","id":"example-1-linear-regression-lm---sales-of-child-car-seats","dir":"Articles","previous_headings":"Some Examples in Action!","what":"Example 1: Linear Regression (lm) - Sales of Child Car Seats","title":"The statlingua Package","text":"Let’s use linear model predict Sales child car seats various predictors using Carseats data set package ISLR2. make example bit complicated, ’ll include pairwise interaction effects model (can include polynomial terms, smoothing splines, type transformation makes sense). Note categorical variables ShelveLoc, Urban, US dummy encoded default). next code chunk loads statlingua package establishes connection (default) Google Gemini model. also define context LLM use explaining output: Next, let’s use Google Gemini model generate explanation model’s output, targeting \"student\" audience \"detailed\" verbosity.","code":"data(Carseats, package = \"ISLR2\")  # load the Carseats data  # Fit a linear model to the Carseats data set fm_carseats <- lm(Sales ~ . + Price:Age + Income:Advertising, data = Carseats) summary(fm_carseats)  # print model summary #>  #> Call: #> lm(formula = Sales ~ . + Price:Age + Income:Advertising, data = Carseats) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -2.9208 -0.7503  0.0177  0.6754  3.3413  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)         6.5755654  1.0087470   6.519 2.22e-10 *** #> CompPrice           0.0929371  0.0041183  22.567  < 2e-16 *** #> Income              0.0108940  0.0026044   4.183 3.57e-05 *** #> Advertising         0.0702462  0.0226091   3.107 0.002030 **  #> Population          0.0001592  0.0003679   0.433 0.665330     #> Price              -0.1008064  0.0074399 -13.549  < 2e-16 *** #> ShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 *** #> ShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 *** #> Age                -0.0579466  0.0159506  -3.633 0.000318 *** #> Education          -0.0208525  0.0196131  -1.063 0.288361     #> UrbanYes            0.1401597  0.1124019   1.247 0.213171     #> USYes              -0.1575571  0.1489234  -1.058 0.290729     #> Price:Age           0.0001068  0.0001333   0.801 0.423812     #> Income:Advertising  0.0007510  0.0002784   2.698 0.007290 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 1.011 on 386 degrees of freedom #> Multiple R-squared:  0.8761, Adjusted R-squared:  0.8719  #> F-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16 library(statlingua)  # Establish client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\".  # Additional context for the LLM to consider carseats_context <- \" The model uses a data set on child car seat sales (in thousands of units) at 400 different stores. The goal is to identify factors associated with sales. The variables are:   * Sales: Unit sales (in thousands) at each location (the response variable).   * CompPrice: Price charged by competitor at each location.   * Income: Community income level (in thousands of dollars).   * Advertising: Local advertising budget for the company at each location (in thousands of dollars).   * Population: Population size in the region (in thousands).   * Price: Price the company charges for car seats at each site.   * ShelveLoc: A factor with levels 'Bad', 'Good', and 'Medium' indicating the quality of the shelving location for the car seats. ('Bad' is the reference level).   * Age: Average age of the local population.   * Education: Education level at each location.   * Urban: A factor ('No', 'Yes') indicating if the store is in an urban or rural location. ('No' is the reference level).   * US: A factor ('No', 'Yes') indicating if the store is in the US or not. ('No' is the reference level). Interaction terms `Income:Advertising` and `Price:Age` are also included. The data set is simulated. We want to understand key drivers of sales and how to interpret the interaction terms. \" explain(fm_carseats, client = client, context = carseats_context,         audience = \"novice\", verbosity = \"detailed\")"},{"path":"/articles/statlingua.html","id":"interpretation-of-linear-regression-model-output","dir":"Articles","previous_headings":"","what":"Interpretation of Linear Regression Model Output","title":"The statlingua Package","text":"output details linear regression model analyzing car seat sales based various factors. model aims understand competitor price, income, advertising, population, car seat price, shelving location, age, education, urban/rural location, US location, interactions price & age income & advertising relate car seat sales. Given response variable (Sales) continuous, linear regression model potentially appropriate, assuming relationships approximately linear assumptions met.","code":""},{"path":"/articles/statlingua.html","id":"call","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Call","title":"The statlingua Package","text":"R command used fit linear regression model. indicates model predicting Sales using variables Carseats dataset, along interaction terms Price:Age Income:Advertising.","code":"lm(formula = Sales ~ . + Price:Age + Income:Advertising, data = Carseats)"},{"path":"/articles/statlingua.html","id":"residuals-summary","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Residuals Summary","title":"The statlingua Package","text":"section describes distribution residuals (differences observed predicted values Sales). * Min: smallest residual -2.9208. * 1Q: 25% residuals less -0.7503. * Median: median residual 0.0177, close zero, suggesting model reasonably well-centered. * 3Q: 75% residuals less 0.6754. * Max: largest residual 3.3413. good model, residuals approximately symmetrically distributed around zero. fact median close zero good sign, important also examine histogram density plot residuals assess overall distribution.","code":"Min      1Q  Median      3Q     Max  -2.9208 -0.7503  0.0177  0.6754  3.3413"},{"path":"/articles/statlingua.html","id":"coefficients-table","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Coefficients Table","title":"The statlingua Package","text":"table presents estimated coefficients predictor model, along standard errors, t-values, p-values. (Intercept): estimated sales predictor variables zero 6.5755654 thousand units. unlikely practically meaningful, represents store $0 competitor price, $0 income, $0 advertising budget, etc. CompPrice: every $1 increase competitor’s price, car seat sales estimated increase 0.0929371 thousand units (92.9371 units), holding variables constant. p-value small, indicating statistically significant positive relationship. Income: every $1,000 increase community income, car seat sales estimated increase 0.0108940 thousand units (10.894 units), holding variables constant. p-value small, indicating statistically significant positive relationship. Advertising: every $1,000 increase local advertising budget, car seat sales estimated increase 0.0702462 thousand units (70.2462 units), holding variables constant. p-value small (0.002030), indicating statistically significant positive relationship. Population: coefficient population small (0.0001592) p-value large (0.665330), suggesting population size statistically significant predictor car seat sales model. every 1,000 person increase population, model estimates increase approximately 0.1592 units, holding variables constant. Price: every $1 increase car seat price, car seat sales estimated decrease 0.1008064 thousand units (100.8064 units), holding variables constant. p-value small, indicating statistically significant negative relationship. ShelveLocGood: Compared ‘Bad’ shelving location, ‘Good’ shelving location associated estimated increase 4.8486762 thousand units car seat sales, holding variables constant. p-value small, indicating statistically significant effect. ShelveLocMedium: Compared ‘Bad’ shelving location, ‘Medium’ shelving location associated estimated increase 1.9532620 thousand units car seat sales, holding variables constant. p-value small, indicating statistically significant effect. Age: every one-year increase average age local population, car seat sales estimated decrease 0.0579466 thousand units (57.9466 units), holding variables constant. p-value small (0.000318), indicating statistically significant negative relationship. Education: coefficient education -0.0208525 p-value 0.288361. suggests education level statistically significant predictor car seat sales model. UrbanYes: coefficient UrbanYes 0.1401597 p-value 0.213171. suggests urban area statistically significant predictor car seat sales model compared rural area. USYes: coefficient USYes -0.1575571 p-value 0.290729. suggests US statistically significant predictor car seat sales model compared US. Price:Age: interaction term. coefficient (0.0001068) represents effect price sales changes age. positive coefficient suggests age increases, negative impact price sales decreases (becomes less negative). p-value 0.423812, indicating interaction statistically significant. illustrate, consider two locations: Location average age 20 Location B average age 60. Increasing price $1 negative impact sales Location Location B. Specifically, difference effect $1 price increase location B location 0.0001068 * (60-20) = 0.004272 thousand units (4.272 units). Income:Advertising: interaction term. coefficient (0.0007510) represents effect income sales changes advertising. positive coefficient suggests advertising increases, positive impact income sales increases. p-value small (0.007290), indicating interaction statistically significant. illustrate, consider two stores: Store C advertising budget $2,000 Store D advertising budget $10,000. Increasing average income $1,000 positive impact sales Store D Store C. Specifically, difference effect $1,000 income increase store D store C 0.0007510 * (10-2) = 0.006008 thousand units (6.008 units).","code":""},{"path":"/articles/statlingua.html","id":"signif--codes","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Signif. codes","title":"The statlingua Package","text":"codes indicate level statistical significance coefficient’s p-value. *** indicates p-value less 0.001 ** indicates p-value less 0.01 * indicates p-value less 0.05 . indicates p-value less 0.1.","code":"Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"/articles/statlingua.html","id":"residual-standard-error","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Residual Standard Error","title":"The statlingua Package","text":"residual standard error (RSE) 1.011. represents typical size prediction errors (average distance observed values fall regression line), measured units response variable (thousands units car seat sales). degrees freedom 386 (number observations minus number coefficients estimated).","code":"Residual standard error: 1.011 on 386 degrees of freedom"},{"path":"/articles/statlingua.html","id":"r-squared","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"R-squared","title":"The statlingua Package","text":"Multiple R-squared: 0.8761 means approximately 87.61% variance car seat sales explained predictor variables model. Adjusted R-squared: 0.8719 modified version R-squared adjusts number predictors model. generally preferred comparing models different numbers predictors. , suggests model explains 87.19% variance car seat sales, accounting model’s complexity.","code":"Multiple R-squared:  0.8761,    Adjusted R-squared:  0.8719"},{"path":"/articles/statlingua.html","id":"f-statistic","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"F-statistic","title":"The statlingua Package","text":"F-statistic tests overall significance model. small p-value (less 2.2e-16) indicates least one predictor variables significantly related car seat sales. F-statistic 210, 13 degrees freedom numerator (number predictors) 386 degrees freedom denominator (number observations minus number predictors minus 1).","code":"F-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16"},{"path":"/articles/statlingua.html","id":"suggestions-for-checking-assumptions","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Suggestions for Checking Assumptions","title":"The statlingua Package","text":"ensure validity linear regression model, ’s crucial check key assumptions: Linearity: Create scatterplots Sales continuous predictor (e.g., CompPrice, Income, Advertising, Population, Price, Age, Education). Also, create plot residuals versus fitted values. Look non-linear patterns plots. non-linearity suspected particular predictor, consider adding polynomial terms transformations predictor. Independence Errors: data time component spatial structure, consider whether errors might correlated. Durbin-Watson test can used formally test autocorrelation. case, since data different stores, assumption likely reasonable, unless spatial correlation stores. Homoscedasticity (Constant Variance Errors): Examine plot residuals versus fitted values Scale-Location plot. Look funnel shape (indicating non-constant variance). Formal tests like Breusch-Pagan White test can also used. Normality Errors: Create Normal Q-Q plot residuals histogram density plot residuals. points Q-Q plot fall approximately along straight line. histogram/density plot look approximately bell-shaped. Shapiro-Wilk test can used formally test normality, visual inspection plots often informative. Multicollinearity: Check high correlations predictor variables. Calculate Variance Inflation Factors (VIFs). VIFs greater 5 10 suggest problematic multicollinearity. Influential Observations/Outliers: Examine plot residuals versus leverage Cook’s distance plot identify influential observations disproportionately affect model. Disclaimer: explanation generated Large Language Model. Critically review output consult additional statistical resources experts ensure correctness full understanding, especially given interpretation relies provided output context.","code":""},{"path":"/articles/statlingua.html","id":"follow-up-question-interpreting-r-squared","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output > Suggestions for Checking Assumptions","what":"Follow-up Question: Interpreting R-squared","title":"The statlingua Package","text":"initial explanation great, let’s say student wants understand R-squared deeply particular model. can use $chat() method client (ellmer \"Chat\" object), remembers context previous interaction. LLM provided detailed explanation R-squared, tailored fm_carseats model provided context, discussing much variability Sales explained predictors model.","code":"query <- paste(\"Could you explain the R-squared values (Multiple R-squared and\",                \"Adjusted R-squared) in simpler terms for this car seat sales\",                \"model? What does it practically mean for predicting sales?\") client$chat(query) #> [1] \"## Simplified Explanation of R-squared Values\\n\\nLet's break down what R-squared and Adjusted R-squared mean in the context of predicting car seat sales:\\n\\n**Multiple R-squared: 0.8761 (or 87.61%)**\\n\\nThink of it like this: Imagine you're trying to guess the car seat sales at a store.\\n\\n*   **Without any information:** If you had absolutely no information about the store (competitor prices, income levels, advertising budgets, etc.), your guesses would likely be pretty far off. There'd be a lot of variation (or \\\"variance\\\") in your prediction errors.\\n\\n*   **With the model:** Now, using our linear regression model (which includes all the predictors like competitor price, income, etc.), we can make much better predictions.\\n\\nR-squared tells us how much *better* our predictions are with the model compared to just guessing randomly. An R-squared of 0.8761 means that *our model explains 87.61% of the total variation (or variance) in car seat sales across all the stores in our data*. In other words, 87.61% of the differences in sales figures we see between stores can be attributed to the factors included in our model (competitor price, income, advertising, etc.). The remaining 12.39% (100% - 87.61%) is due to other factors *not* included in the model (e.g., random chance, unmeasured variables, store manager skill, local events, etc.).\\n\\n**In simpler terms:** The model captures a large proportion of the reasons why sales vary from store to store.\\n\\n**Adjusted R-squared: 0.8719 (or 87.19%)**\\n\\nAdjusted R-squared is a slightly more refined version of R-squared. The regular R-squared *always* increases when you add more variables to your model, even if those variables don't really help predict sales. It's like saying, \\\"I can explain sales even better if I add the color of the store's walls to the model!\\\" (Even though wall color is probably irrelevant).\\n\\nAdjusted R-squared *penalizes* you for adding useless variables. It only increases if the new variable actually improves the model's predictive power *more than you'd expect by chance*.\\n\\nIn this case, the Adjusted R-squared (0.8719) is very close to the regular R-squared (0.8761). This suggests that the variables included in our model are, on the whole, useful for predicting sales. The penalty for including 13 variables was very small.\\n\\n**Practical Meaning for Predicting Sales:**\\n\\n*   **Good predictive power:** An Adjusted R-squared of 0.8719 indicates that the model has relatively strong predictive power. We can use the model to estimate car seat sales at new stores, and these estimates are likely to be fairly accurate. However, it is essential to remember that there is still 12.81% of the variation that the model *doesn't* explain.\\n\\n*   **Identifying key factors:** The model helps us identify which factors are most strongly associated with car seat sales (e.g., competitor price, the company's own price, shelf location). This knowledge can be used to make strategic decisions about pricing, advertising, and product placement to improve sales.\\n\\n*   **Not perfect prediction:** Even with a high R-squared, the model doesn't predict sales *perfectly*. There will still be some error in our predictions. Other factors not included in the model also influence sales.\\n\\n*   **Useful for comparisons:** The R-squared values are especially useful for comparing this model to other models you might build to predict car seat sales. The model with the higher Adjusted R-squared (assuming both models meet the regression assumptions) is generally considered the better model.\\n\""},{"path":"/articles/statlingua.html","id":"example-2-logistic-glm-glm---pima-indians-diabetes","dir":"Articles","previous_headings":"Interpretation of Linear Regression Model Output","what":"Example 2: Logistic GLM (glm) - Pima Indians Diabetes","title":"The statlingua Package","text":"Let’s use Pima.tr data set MASS package fit logistic regression model. data set prevalence diabetes Pima Indian women. goal identify factors associated likelihood testing positive diabetes. Now, let’s provide additional context accompany output requesting explanation LLM: time, ’ll ask statlingua explanation, targeting \"researcher\" \"moderate\" verbosity. audience interested aspects like odds ratios model fit.","code":"data(Pima.tr, package = \"MASS\")  # load the Pima.tr data set  # Fit a logistic regression model fm_pima <- glm(type ~ npreg + glu + bp + skin + bmi + ped + age,                data = Pima.tr, family = binomial(link = \"logit\")) summary(fm_pima)  # print model summary #>  #> Call: #> glm(formula = type ~ npreg + glu + bp + skin + bmi + ped + age,  #>     family = binomial(link = \"logit\"), data = Pima.tr) #>  #> Deviance Residuals:  #>     Min       1Q   Median       3Q      Max   #> -1.9830  -0.6773  -0.3681   0.6439   2.3154   #>  #> Coefficients: #>              Estimate Std. Error z value Pr(>|z|)     #> (Intercept) -9.773062   1.770386  -5.520 3.38e-08 *** #> npreg        0.103183   0.064694   1.595  0.11073     #> glu          0.032117   0.006787   4.732 2.22e-06 *** #> bp          -0.004768   0.018541  -0.257  0.79707     #> skin        -0.001917   0.022500  -0.085  0.93211     #> bmi          0.083624   0.042827   1.953  0.05087 .   #> ped          1.820410   0.665514   2.735  0.00623 **  #> age          0.041184   0.022091   1.864  0.06228 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 256.41  on 199  degrees of freedom #> Residual deviance: 178.39  on 192  degrees of freedom #> AIC: 194.39 #>  #> Number of Fisher Scoring iterations: 5 pima_context <- \" This logistic regression model attempts to predict the likelihood of a Pima Indian woman testing positive for diabetes. The data is from a study on women of Pima Indian heritage, aged 21 years or older, living near Phoenix, Arizona. The response variable 'type' is binary: 'Yes' (tests positive for diabetes) or 'No'.  Predictor variables include:   - npreg: Number of pregnancies.   - glu: Plasma glucose concentration in an oral glucose tolerance test.   - bp: Diastolic blood pressure (mm Hg).   - skin: Triceps skin fold thickness (mm).   - bmi: Body mass index (weight in kg / (height in m)^2).   - ped: Diabetes pedigree function (a measure of genetic predisposition).   - age: Age in years.  The goal is to understand which of these factors are significantly associated with an increased or decreased odds of having diabetes. We are particularly interested in interpreting coefficients as odds ratios. \"  # Establish fresh client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". explain(fm_pima, client = client, context = pima_context,         audience = \"researcher\", verbosity = \"moderate\")"},{"path":"/articles/statlingua.html","id":"explanation-of-the-binomial-glm-with-logit-link-output","dir":"Articles","previous_headings":"","what":"Explanation of the Binomial GLM with Logit Link Output","title":"The statlingua Package","text":"output represents logistic regression model, type Generalized Linear Model (GLM), used predict probability binary outcome (case, diabetes status). model uses binomial family logit link function. Core Concepts: Binomial Family Logit Link: combination appropriate binary (0/1 Yes/) outcome variables. logit link transforms probability event (diabetes) log-odds event. model predicts log-odds linear combination predictor variables. Key Assumptions: Independence: diabetes status woman independent others. Linearity Log-Odds: relationship predictors log-odds diabetes linear. Correctly Specified Variance: variance binary outcome determined binomial distribution. Overdispersion: variance assumed predicted binomial distribution; isn’t variance expected. Assessing Model Appropriateness: Given outcome variable ‘type’ binary, binomial family logit link generally appropriate. rely correctness assumptions given context. Overdispersion possible issue, can check based deviance relative degrees freedom. Interpretation glm() Output: Call: glm(formula = type ~ npreg + glu + bp + skin + bmi + ped + age,         family = binomial(link = \"logit\"), data = Pima.tr) R command used fit model. Deviance Residuals: Min       1Q   Median       3Q      Max     -1.9830  -0.6773  -0.3681   0.6439   2.3154 Deviance residuals measure difference observed predicted values. analogous residuals linear regression. Ideally, symmetrically distributed around zero. , distribution appears reasonably symmetric. Coefficients Table: row table provides information effect predictor log-odds testing positive diabetes. Estimate: estimated coefficient log-odds scale. Std. Error: standard error estimated coefficient. z value: test statistic (Estimate / Std. Error). Pr(>|z|): p-value associated z-test. represents probability observing z-value extreme extreme one calculated, assuming null hypothesis (coefficient zero) true. Odds Ratio: exponentiated estimate (exp(Estimate)). multiplicative change odds diabetes one-unit increase predictor, holding variables constant. odds ratio greater 1 indicates positive association odds diabetes, odds ratio less 1 indicates negative association. Signif. codes: linear regression, indicate level statistical significance coefficients. (Dispersion parameter binomial family taken 1): binomial family, dispersion parameter fixed 1. overdispersion (variance expected), value greater 1, one might consider using quasibinomial family. Null deviance: Null deviance: 256.41  199  degrees freedom represents deviance model intercept (.e., predictors). indicates total variability response variable. degrees freedom n - 1, n number observations. Residual deviance: Residual deviance: 178.39  192  degrees freedom represents deviance fitted model. indicates amount variability explained model. degrees freedom n - p, n number observations p number parameters model (including intercept). AIC: AIC: 194.39 Akaike Information Criterion (AIC) measure model fit penalizes model complexity. Lower AIC values indicate better trade-fit complexity. ’s useful comparing different models. Number Fisher Scoring iterations: Number Fisher Scoring iterations: 5 indicates iterative algorithm used fit model converged 5 iterations. Suggestions Checking Assumptions: Deviance/Pearson Residuals vs. Fitted Values Plot: Plotting deviance Pearson residuals fitted values (predicted log-odds) can help assess whether variance constant whether link function appropriate. Look patterns trends residuals. Influential Observations: Check observations disproportionate influence model’s results. Overdispersion: residual deviance (178.39) less residual degrees freedom (192), suggesting overdispersion major concern case. rough rule thumb substantial overdispersion possible residual deviance substantially larger residual degrees freedom (e.g., residual deviance twice residual df). Additional Considerations: Overdispersion: overdispersion present, standard errors underestimated, potentially leading incorrect inferences. suspected, use quasibinomial model investigate potential sources extra variation. Zero-Inflation: relevant specific model, outcome binary variable count data. Caution: explanation generated Large Language Model. Critically review output consult additional statistical resources experts ensure correctness full understanding. (rendered Markdown) explains logistic regression coefficients (e.g., glu bmi) terms log-odds odds ratios, discusses statistical significance, interprets overall model fit statistics like AIC deviance. researcher, explanation might also touch upon implications findings diabetes risk assessment. Thank catching error! ’s crucial accurate reliable examples. Pima.tr data set much suitable choice GLM example.","code":""},{"path":"/articles/statlingua.html","id":"example-3-cox-proportional-hazards-model-coxph---lung-cancer-survival","dir":"Articles","previous_headings":"Explanation of the Binomial GLM with Logit Link Output","what":"Example 3: Cox Proportional Hazards Model (coxph) - Lung Cancer Survival","title":"The statlingua Package","text":"Let’s model patient survival lung cancer study using lung data set survival package. classic data set Cox PH models. ’s additional context provide lung cancer survival model: Let’s get explanation \"manager\" audience, looking \"brief\" overview.","code":"library(survival)  # Load the lung cancer data set (from package survival) data(cancer)  # Fit a time transform Cox PH model using current age fm_lung <- coxph(Surv(time, status) ~ ph.ecog + tt(age), data = lung,      tt = function(x, t, ...) pspline(x + t/365.25)) summary(fm_lung)  # print model summary #> Call: #> coxph(formula = Surv(time, status) ~ ph.ecog + tt(age), data = lung,  #>     tt = function(x, t, ...) pspline(x + t/365.25)) #>  #>   n= 227, number of events= 164  #>    (1 observation deleted due to missingness) #>  #>                 coef    se(coef) se2      Chisq DF   p       #> ph.ecog         0.45284 0.117827 0.117362 14.77 1.00 0.00012 #> tt(age), linear 0.01116 0.009296 0.009296  1.44 1.00 0.23000 #> tt(age), nonlin                            2.70 3.08 0.45000 #>  #>                    exp(coef) exp(-coef) lower .95 upper .95 #> ph.ecog                1.573     0.6358    1.2484     1.981 #> ps(x + t/365.25)3      1.275     0.7845    0.2777     5.850 #> ps(x + t/365.25)4      1.628     0.6141    0.1342    19.761 #> ps(x + t/365.25)5      2.181     0.4585    0.1160    41.015 #> ps(x + t/365.25)6      2.762     0.3620    0.1389    54.929 #> ps(x + t/365.25)7      2.935     0.3408    0.1571    54.812 #> ps(x + t/365.25)8      2.843     0.3517    0.1571    51.472 #> ps(x + t/365.25)9      2.502     0.3997    0.1382    45.310 #> ps(x + t/365.25)10     2.529     0.3955    0.1390    45.998 #> ps(x + t/365.25)11     3.111     0.3214    0.1699    56.961 #> ps(x + t/365.25)12     3.610     0.2770    0.1930    67.545 #> ps(x + t/365.25)13     5.487     0.1822    0.2503   120.280 #> ps(x + t/365.25)14     8.903     0.1123    0.2364   335.341 #>  #> Iterations: 4 outer, 10 Newton-Raphson #>      Theta= 0.7960256  #> Degrees of freedom for terms= 1.0 4.1  #> Concordance= 0.612  (se = 0.027 ) #> Likelihood ratio test= 22.46  on 5.07 df,   p=5e-04 lung_context <- \" This Cox proportional hazards model analyzes survival data for patients with advanced lung cancer. The objective is to identify factors associated with patient survival time (in days). The variables include:   - time: Survival time in days.   - status: Censoring status (1=censored, 2=dead).   - age: Age in years.   - sex: Patient's sex (1=male, 2=female). Note: In the model, 'sex' is treated as numeric; interpretations should consider this. It's common to factor this, but here it's numeric.   - ph.ecog: ECOG performance score (0=good, higher values mean worse performance). We want to understand how age, sex, and ECOG score relate to the hazard of death. Interpretations should focus on hazard ratios. For example, how does a one-unit increase in ph.ecog affect the hazard of death? \"  # Establish fresh client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". explain(fm_lung, client = client, context = lung_context,         audience = \"manager\", verbosity = \"brief\")"},{"path":"/articles/statlingua.html","id":"explanation-of-cox-proportional-hazards-model-output","dir":"Articles","previous_headings":"","what":"Explanation of Cox Proportional Hazards Model Output","title":"The statlingua Package","text":"output presents results Cox proportional hazards model examining survival times lung cancer patients. model explores relationship survival time ph.ecog (ECOG performance score) age accounting possible non-linear effects age via time-transforming spline (tt(age)). Call: model fit using formula Surv(time, status) ~ ph.ecog + tt(age), tt(age) represents time-transforming function using spline age. Data Summary: n = 227 patients included analysis (one observation removed due missing data). number events = 164 deaths observed. Coefficients Table: coef: 0.45284. estimated coefficient log-hazard scale. positive coefficient suggests higher ECOG score associated increased hazard death. exp(coef) (Hazard Ratio - HR): 1.573. one-unit increase ECOG performance score (indicating poorer performance), hazard death estimated 1.573 times higher, holding age constant. translates 57.3% higher risk death. se(coef): 0.117827. Standard error coefficient. z: 14.77. Wald test statistic. Pr(>|z|) p: 0.00012. p-value highly significant, indicating strong evidence ECOG score associated survival time. lower .95: 1.2484, upper .95: 1.981. 95% confidence interval hazard ratio (1.2484, 1.981). interval include 1, effect ph.ecog statistically significant 0.05 level. output presents linear non-linear component spline. linear component statistically significant (p = 0.23000). non-linear component Chisq value 2.70 3.08 degrees freedom p-value 0.45, suggesting non-linear component also statistically significant. hazard ratios confidence intervals ps(x + t/365.25)3 ps(x + t/365.25)14 represent effect spline basis functions. Since age included tt(), ’s allowed time-varying effect. individual hazard ratios spline terms difficult interpret isolation; overall significance spline informative. Given non-significance non-linear component, simpler model without spline might sufficient. Model Convergence: model converged 4 outer 10 Newton-Raphson iterations. Theta= 0.7960256 refers parameter penalized spline fitting process. Model Fit Significance: Degrees freedom terms: Reports degrees freedom term. Concordance= 0.612 (se = 0.027 ): concordance 0.612, standard error 0.027. indicates approximately 61.2% pairs patients, model correctly predicts patient experience event (death) sooner. better chance (0.5), suggests moderate discriminative ability. Likelihood ratio test= 22.46 5.07 df, p=5e-04: likelihood ratio test compares fitted model null model (without predictors). small p-value (0.0005) suggests model ph.ecog spline age significantly better model predictors. Recommendations Cautions: Proportional Hazards Assumption: Given time-transforming spline age, proportional hazards assumption age assumed. However, crucial assess proportional hazards assumption ph.ecog. can done using Schoenfeld residuals via cox.zph(). Violations assumption suggest effect ph.ecog changes time. Spline Interpretation: overall likelihood ratio test significant, individual components age spline . Consider visualizing effect spline using plots understand age might affecting hazard time. Given non-significance non-linear component age spline, consider refitting model without spline (.e., standard linear effect age). Model Simplicity: time-varying effect age critical, simplifying model removing spline modeling age linear term may improve interpretability without sacrificing much predictive power. Review Validate: Critically review interpretation consult statistical resources experts ensure correctness full understanding model, particularly given use splines. rendered Markdown output provides concise, high-level summary suitable manager, focusing key predictors survival implications terms increased decreased risk (hazard).","code":""},{"path":"/articles/statlingua.html","id":"example-4-linear-mixed-effects-model-lmer-from-lme4---sleep-study","dir":"Articles","previous_headings":"Explanation of Cox Proportional Hazards Model Output","what":"Example 4: Linear Mixed-Effects Model (lmer from lme4) - Sleep Study","title":"The statlingua Package","text":"Let’s explore sleepstudy data set lme4 package. data set records average reaction time per day subjects sleep deprivation study. ’ll fit linear mixed-effects model see reaction time changes days sleep deprivation, accounting random variation among subjects. example also demonstrate style argument, requesting output plain text (style = \"text\") JSON string (style = \"json\"). Now, let’s define context sleep study model:","code":"library(lme4)  # Load the sleep study data set data(sleepstudy)  # Fit a linear mixed-effects model allowing for random intercepts and random # slopes for Days, varying by Subject fm_sleep <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy) summary(fm_sleep)  # print model summary #> Linear mixed model fit by REML ['lmerMod'] #> Formula: Reaction ~ Days + (Days | Subject) #>    Data: sleepstudy #>  #> REML criterion at convergence: 1743.6 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.9536 -0.4634  0.0231  0.4634  5.1793  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr #>  Subject  (Intercept) 612.10   24.741        #>           Days         35.07    5.922   0.07 #>  Residual             654.94   25.592        #> Number of obs: 180, groups:  Subject, 18 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  251.405      6.825  36.838 #> Days          10.467      1.546   6.771 #>  #> Correlation of Fixed Effects: #>      (Intr) #> Days -0.138 sleepstudy_context <- \" This linear mixed-effects model analyzes data from a sleep deprivation study. The goal is to understand the effect of days of sleep deprivation ('Days') on average reaction time ('Reaction' in ms). The model includes random intercepts and random slopes for 'Days' for each 'Subject', acknowledging that baseline reaction times and the effect of sleep deprivation may vary across individuals. We are interested in the average fixed effect of an additional day of sleep deprivation on reaction time, as well as the extent of inter-subject variability. \"  # Establish fresh client connection client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\"."},{"path":"/articles/statlingua.html","id":"requesting-plain-text-output-style-text","dir":"Articles","previous_headings":"Explanation of Cox Proportional Hazards Model Output > Example 4: Linear Mixed-Effects Model (lmer from lme4) - Sleep Study","what":"Requesting Plain Text Output (style = \"text\")","title":"The statlingua Package","text":"Let’s ask statlingua explanation plain text, targeting \"researcher\" \"moderate\" verbosity.","code":"explain(fm_sleep, client = client, context = sleepstudy_context,         audience = \"researcher\", verbosity = \"moderate\", style = \"text\") #> Here's an explanation of the linear mixed-effects model output, incorporating the context of the sleep deprivation study. #>  #> LINEAR MIXED-EFFECTS MODEL INTERPRETATION #>  #> The model examines the relationship between days of sleep deprivation (Days) and reaction time (Reaction), accounting for individual differences between subjects.  The formula `Reaction ~ Days + (Days | Subject)` indicates that the model includes a fixed effect of 'Days' on 'Reaction' and random intercepts and slopes for 'Days' varying across 'Subject'. #>  #> REML VS. ML #>  #> The model was fit using REML (Restricted Maximum Likelihood), which is generally preferred for estimating variance components in mixed-effects models. #>  #> RANDOM EFFECTS #>  #> This section describes the variability between subjects. #>  #> *   Subject (Intercept): Variance = 612.10, Std.Dev. = 24.741. This indicates substantial inter-individual variability in baseline reaction times (i.e., at Day 0 of sleep deprivation).  The standard deviation of 24.741 ms suggests that, even before sleep deprivation, subjects' reaction times vary by approximately ±24.741 ms. #>  #> *   Subject Days: Variance = 35.07, Std.Dev. = 5.922. This indicates inter-individual variability in the effect of sleep deprivation on reaction time.  Some subjects' reaction times increase more per day of sleep deprivation than others. The standard deviation of 5.922 ms/day suggests that the effect of each additional day of sleep deprivation varies by approximately ±5.922 ms/day across subjects. #>  #> *   Corr: 0.07. This is the correlation between random intercepts and random slopes.  A small, positive correlation suggests a weak relationship between a subject's baseline reaction time and how much their reaction time changes with sleep deprivation. Subjects with slightly higher baseline reaction times tend to show a slightly larger increase in reaction time per day of sleep deprivation, although this correlation is very weak. #>  #> *   Residual: Variance = 654.94, Std.Dev. = 25.592. This represents the within-subject variability in reaction time that is not explained by the fixed effect of 'Days' or the random effects. This is the variability of reaction time *after* accounting for the subject-specific baselines and slopes. The standard deviation of 25.592 ms represents the typical variation in reaction time for a given subject on a given day. #>  #> FIXED EFFECTS #>  #> This section describes the average effects across all subjects. #>  #> *   (Intercept): Estimate = 251.405, Std. Error = 6.825, t value = 36.838. This is the estimated average reaction time (in ms) at Day 0 of sleep deprivation, across all subjects. The t-value is very large, indicating that this average baseline reaction time is significantly different from zero. #>  #> *   Days: Estimate = 10.467, Std. Error = 1.546, t value = 6.771. This is the estimated average increase in reaction time (in ms) for each additional day of sleep deprivation, across all subjects. On average, a subject's reaction time increases by 10.467 ms for each day of sleep deprivation. The t-value is large, indicating a statistically significant effect of days of sleep deprivation on reaction time. #>  #> CORRELATION OF FIXED EFFECTS #>  #> *   Days -0.138: This is the correlation between the intercept and the slope estimates in the fixed effects part of the model. It's generally less important to interpret than the correlation of random effects. It indicates a slight negative relationship between the estimated average baseline reaction time and the estimated average effect of days. #>  #> MODEL APPROPRIATENESS (BASED ON CONTEXT) #>  #> Given the repeated measures design (multiple observations per subject), the linear mixed-effects model is appropriate as it accounts for the non-independence of observations within each subject.  The inclusion of both random intercepts and random slopes allows for individual differences in both baseline reaction times and the effect of sleep deprivation, which is a reasonable assumption.  The model assumes linearity between days of sleep deprivation and reaction time. #>  #> CHECKING ASSUMPTIONS #>  #> To further validate the model: #>  #> *   Examine a Q-Q plot of the residuals to assess normality. #> *   Plot residuals against fitted values to check for homoscedasticity (constant variance). #> *   Examine Q-Q plots of the random effects for each subject to assess normality of random effects. #> *   Plot residuals against 'Days' to assess the linearity assumption. #>  #> CAUTION: This explanation was generated by a Large Language Model. Critically review the output and consult additional statistical resources or experts to ensure correctness and a full understanding. #>"},{"path":"/articles/statlingua.html","id":"requesting-json-output-style-json","dir":"Articles","previous_headings":"Explanation of Cox Proportional Hazards Model Output > Example 4: Linear Mixed-Effects Model (lmer from lme4) - Sleep Study","what":"Requesting JSON Output (style = \"json\")","title":"The statlingua Package","text":"Now, let’s request explanation structured JSON format (using style = \"json\"). ’ll target \"student\" \"detailed\" verbosity.","code":"client <- ellmer::chat_google_gemini(echo = \"none\") #> Using model = \"gemini-2.0-flash\". ex <- explain(fm_sleep, client = client, context = sleepstudy_context,               audience = \"student\", verbosity = \"detailed\", style = \"json\")  # The 'text' component of the statlingua_explanation object now holds a JSON # string which can be parsed using the jsonlite package jsonlite::prettify(ex$text) #> { #>     \"title\": \"Explanation of Linear Mixed-Effects Model for Sleep Deprivation Study\", #>     \"model_overview\": \"This is a linear mixed-effects model, fitted using Restricted Maximum Likelihood (REML), examining the impact of sleep deprivation (Days) on reaction time (Reaction). The model accounts for inter-individual variability by including random intercepts and slopes for each subject, allowing for differences in baseline reaction times and how reaction time changes with sleep deprivation across individuals. This type of model is suitable due to the repeated measures design, where we have multiple observations for each subject.\", #>     \"coefficient_interpretation\": \"The fixed effects estimates provide the average effect of each predictor on reaction time across all subjects. The intercept is estimated at 251.405 ms, which represents the average reaction time at Day 0 (the baseline reaction time) across all subjects. The coefficient for 'Days' is 10.467 ms. This indicates that, on average, reaction time increases by 10.467 milliseconds for each additional day of sleep deprivation. So, each day of sleep deprivation increases reaction time by about 10.5 ms.\", #>     \"significance_assessment\": \"The t-value for the intercept is 36.838 and for 'Days' is 6.771. Although the output doesn't directly provide p-values, these t-values can be used to assess statistical significance. Given the large t-values, especially for 'Days', it's highly likely that the effect of days of sleep deprivation on reaction time is statistically significant. The standard error for 'Days' is 1.546, which provides a measure of the precision of the estimated effect of sleep deprivation. Packages like `lmerTest` can be used with this model to obtain p-values for these fixed effects estimates using methods like Satterthwaite or Kenward-Roger approximations.\", #>     \"goodness_of_fit\": \"The REML criterion at convergence is 1743.6. This value is useful for comparing nested models fitted to the same data. Lower REML values indicate a better fit, but only when comparing models with the same fixed effects. Other metrics like AIC and BIC, which can be obtained from the model, are better suited to non-nested model comparisons.\", #>     \"assumptions_check\": \"Several assumptions should be checked to ensure the validity of this model. First, examine the normality of the residuals using a Q-Q plot or histogram of `resid(object)`. Second, assess homoscedasticity by plotting residuals against fitted values (`fitted(object)`), looking for constant variance. Third, check the normality of the random effects using Q-Q plots of the random effects (`ranef(object)`). Additionally, linearity of the fixed effect 'Days' should be checked, possibly by plotting residuals against 'Days'. Finally, it's important to check for influential observations that might disproportionately affect the model results.\", #>     \"key_findings\": \"- On average, baseline reaction time (Day 0) is estimated to be approximately 251.4 ms.\\n- Each additional day of sleep deprivation is associated with an average increase of approximately 10.5 ms in reaction time.\\n- There is substantial inter-individual variability in both baseline reaction time (Std.Dev. = 24.741 ms) and the effect of sleep deprivation (Std.Dev. = 5.922 ms) across subjects.\\n- The correlation between random intercepts and slopes is low (0.07), suggesting a weak relationship between a subject's baseline reaction time and how their reaction time changes with sleep deprivation.\", #>     \"warnings_limitations\": \"The interpretation relies on the assumption that the model is correctly specified and that the assumptions of linear mixed-effects models are met. The absence of p-values in the base output from `lmer` should be addressed using appropriate packages like `lmerTest` to draw firmer conclusions about statistical significance. Also, remember that correlation does not equal causation; while the model suggests a relationship between sleep deprivation and reaction time, it does not prove that sleep deprivation *causes* the change in reaction time.\" #> } #>"},{"path":"/articles/statlingua.html","id":"inspecting-llm-interaction","dir":"Articles","previous_headings":"","what":"Inspecting LLM Interaction","title":"The statlingua Package","text":"want see exact system user prompts statlingua generated sent LLM (via ellmer), well raw response LLM, can print ellmer \"Chat\" object (defined client example) explain() call. client object stores history interaction. transparency invaluable debugging, understanding process, even refining prompts developing custom extensions statlingua.","code":"print(client) #> <Chat Google/Gemini/gemini-2.0-flash turns=3 tokens=2337/887 $0.00> #> ── system [0] ─────────────────────────────────────────────────────────────────────────────────────── #> ## Role #>  #> You are an expert statistician and R programmer, gifted at explaining complex concepts simply. Your primary function is to interpret statistical model outputs from R. You understand model nuances, underlying assumptions, and how results relate to real-world research questions. #>  #> You are particularly skilled with **Linear Mixed-Effects Models** created using the `lmer()` function from the `lme4` package in R (producing `lmerMod` objects). You understand their estimation via REML or ML, the interpretation of fixed and random effects (including correlations), and the common practice of using packages like `lmerTest` for p-value estimation. #>  #>  #> ## Intended Audience and Verbosity #>  #> ### Target Audience: Student #> Assume the user is learning statistics. Explain concepts thoroughly but clearly, as if teaching. Define key terms as they arise and explain *why* certain statistics are important or what they indicate in the context of the analysis. Maintain an encouraging and educational tone. #>  #> ### Level of Detail (Verbosity): Detailed #> Give a comprehensive interpretation. Include nuances of the model output, a detailed breakdown of all relevant statistics, and a thorough discussion of assumptions and diagnostics. Be as thorough as possible, exploring various facets of the results. #>  #>  #> ## Response Format Specification (Style: Json) #>  #> Your response MUST be a valid JSON object that can be parsed directly into an R list. #> The JSON object should have the following top-level keys, each containing a string with the relevant part of the explanation (formatted as plain text or simple Markdown within the string if appropriate for that section): #> - \"title\": A concise title for the explanation. #> - \"model_overview\": A general description of the model type and its purpose in this context. #> - \"coefficient_interpretation\": Detailed interpretation of model coefficients/parameters. #> - \"significance_assessment\": Discussion of p-values, confidence intervals, and statistical significance. #> - \"goodness_of_fit\": Evaluation of model fit (e.g., R-squared, AIC, deviance). #> - \"assumptions_check\": Comments on important model assumptions and how they might be checked. #> - \"key_findings\": A bulleted list (as a single string with newlines `\\n` for bullets) of the main conclusions. #> - \"warnings_limitations\": Any warnings, limitations, or caveats regarding the model or its interpretation. #>  #> Example of expected JSON structure: #> { #>   \"title\": \"Explanation of Linear Regression Model for Car Sales\", #>   \"model_overview\": \"This is a linear regression model...\", #>   \"coefficient_interpretation\": \"The coefficient for 'Price' is -0.10, suggesting that...\", #>   \"significance_assessment\": \"The p-value for 'Price' is very small (< 0.001)...\", #>   \"goodness_of_fit\": \"The R-squared value is 0.87, indicating...\", #>   \"assumptions_check\": \"Assumptions such as linearity and homoscedasticity should be checked by examining residual plots.\", #>   \"key_findings\": \"- Price is a significant negative predictor of sales.\\n- Advertising has a positive impact on sales.\", #>   \"warnings_limitations\": \"This model is based on simulated data and results should be interpreted with caution.\" #> } #>  #> Ensure the entire output is ONLY the JSON object. #> DO NOT wrap your entire response in JSON code fences (e.g., ```json ... ``` or ``` ... ```). #> DO NOT include any conversational pleasantries or introductory/concluding phrases. #>  #>  #> ## Instructions #>  #> You are explaining a **Linear Mixed-Effects Model** (from `lme4::lmer()`, an `lmerMod` object). #>  #> **Core Concepts & Purpose:** #> This model is used for data with hierarchical/nested structures or repeated measures, where observations are not independent. It models fixed effects (average effects of predictors) and random effects (variability between groups/subjects for intercepts and/or slopes). #>  #> **Key Assumptions:** #> * Linearity: The relationship between predictors and the outcome is linear. #> * Independence of random effects and errors: Random effects are independent of each other and of the residuals (conditional on covariates). #> * Normality of random effects: Random effects for each grouping factor are normally distributed (typically with mean 0). #> * Normality of residuals: The errors (residuals) are normally distributed with a mean of 0. #> * Homoscedasticity: Residuals have constant variance. #> * (Note on p-values: `lme4::lmer` itself does not calculate p-values for fixed effects by default due to complexities with degrees of freedom. If p-values are present, they often come from wrapper functions or packages like `lmerTest` using approximations like Satterthwaite's or Kenward-Roger methods.) #>  #> **Assessing Model Appropriateness (Based on User Context):** #> If the user provides context: #> * Comment on the model's appropriateness based on the data structure (e.g., repeated measures, nesting) and research question. #> * Relate this to the model's assumptions. #> If no or insufficient context, state inability to fully assess appropriateness. #>  #> **Interpretation of the `lmerMod` Output:** #> * **Formula and Data:** Briefly reiterate what is being modeled. #> * **REML vs ML:** Note if Restricted Maximum Likelihood (REML) or Maximum Likelihood (ML) was used (REML is default and often preferred for variance components; ML for likelihood ratio tests of fixed effects). #> * **Random Effects (from `VarCorr()` output):** #>     * For each grouping factor (e.g., `(1 | group_factor)` or `(predictor | group_factor)`): #>         * **Variance and Std.Dev. (for intercepts):** Quantifies between-group variability in the baseline outcome. #>         * **Variance and Std.Dev. (for slopes):** Quantifies how much the effect of that predictor varies across groups. #>         * **Correlation of Random Effects (e.g., `Corr` between random intercept and slope for the same group):** Explain its meaning (e.g., a correlation of -0.5 between intercept and slope for 'day' within 'subject' means subjects with higher initial values tend to have a less steep increase over days). #>     * **Residual Variance/Std.Dev.:** Within-group or unexplained variability. #> * **Fixed Effects (from `coef(summary(object))`):** #>     * For each predictor: #>         * **Estimate:** The estimated average effect on the outcome for a one-unit change in the predictor (or difference from the reference level for categorical predictors), accounting for random effects. #>         * **Std. Error:** Precision of the estimate. #>         * **t-value (or z-value):** `Estimate / Std. Error`. #>         * **P-values (if available, typically from `lmerTest` via `summary(as(object, \"lmerModLmerTest\"))` or `anova(object)` from `lmerTest`):** Interpret as the probability of observing such an extreme t-value if the true fixed effect is zero. If p-values are NOT directly in the basic `summary(object)` output, mention they usually come from add-on packages. #> * **ANOVA Table for Fixed Effects (if provided, typically from `lmerTest::anova()`):** #>     * Tests the overall significance of fixed effects. For each term: interpret F-statistic, degrees of freedom (NumDF, DenDF), and p-value. #> * **Model Fit Statistics (AIC, BIC, logLik, deviance):** #>     * Explain as measures for model comparison. Lower AIC/BIC, higher logLik (less negative deviance) generally indicate better relative fit. #>  #> **Suggestions for Checking Assumptions:** #> * **Normality of Residuals:** Suggest Q-Q plot or histogram of residuals (`resid(object)`). #> * **Homoscedasticity:** Suggest plotting residuals vs. fitted values (`fitted(object)`). Look for non-constant spread. #> * **Normality of Random Effects:** Suggest Q-Q plots of the random effects (e.g., from `ranef(object)`). #> * **Linearity (Fixed Effects):** Check by plotting residuals against continuous predictors. #> * **Influential Observations:** Mention checking for influential data points. #> * **Singularity:** If the model is reported as singular (e.g. random effect variances near zero or correlations near +/-1), this often indicates an overly complex random effects structure that the data cannot support. #>  #> **Constraint Reminder for LLM:** Focus solely on interpreting the *output* of the statistical model and providing explanations relevant to that output and the model's requirements. Do not perform new calculations or suggest alternative analyses unless directly prompted by assessing the appropriateness based on provided context. **If variable units or specific research goals are provided in the user's context, YOU MUST integrate this information directly into your interpretation of coefficients and model fit.** #>  #>  #> ## Caution #>  #> This explanation was generated by a Large Language Model. Advise the user to critically review the output and consult additional statistical resources or experts to ensure correctness and a full understanding, especially given that the interpretation relies on the provided output and context. #> ── user [2337] ────────────────────────────────────────────────────────────────────────────────────── #> Explain the following linear mixed-effects model output: #> Linear mixed model fit by REML ['lmerMod'] #> Formula: Reaction ~ Days + (Days | Subject) #>    Data: sleepstudy #>  #> REML criterion at convergence: 1743.6 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.9536 -0.4634  0.0231  0.4634  5.1793  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr #>  Subject  (Intercept) 612.10   24.741        #>           Days         35.07    5.922   0.07 #>  Residual             654.94   25.592        #> Number of obs: 180, groups:  Subject, 18 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  251.405      6.825  36.838 #> Days          10.467      1.546   6.771 #>  #> Correlation of Fixed Effects: #>      (Intr) #> Days -0.138 #>  #> ## Additional context to consider #>  #>  #> This linear mixed-effects model analyzes data from a sleep deprivation study. #> The goal is to understand the effect of days of sleep deprivation ('Days') on #> average reaction time ('Reaction' in ms). The model includes random intercepts #> and random slopes for 'Days' for each 'Subject', acknowledging that baseline #> reaction times and the effect of sleep deprivation may vary across individuals. #> We are interested in the average fixed effect of an additional day of sleep #> deprivation on reaction time, as well as the extent of inter-subject #> variability. #>  #> ── assistant [887] ────────────────────────────────────────────────────────────────────────────────── #> ```json #> { #>   \"title\": \"Explanation of Linear Mixed-Effects Model for Sleep Deprivation Study\", #>   \"model_overview\": \"This is a linear mixed-effects model, fitted using Restricted Maximum Likelihood (REML), examining the impact of sleep deprivation (Days) on reaction time (Reaction). The model accounts for inter-individual variability by including random intercepts and slopes for each subject, allowing for differences in baseline reaction times and how reaction time changes with sleep deprivation across individuals. This type of model is suitable due to the repeated measures design, where we have multiple observations for each subject.\", #>   \"coefficient_interpretation\": \"The fixed effects estimates provide the average effect of each predictor on reaction time across all subjects. The intercept is estimated at 251.405 ms, which represents the average reaction time at Day 0 (the baseline reaction time) across all subjects. The coefficient for 'Days' is 10.467 ms. This indicates that, on average, reaction time increases by 10.467 milliseconds for each additional day of sleep deprivation. So, each day of sleep deprivation increases reaction time by about 10.5 ms.\", #>   \"significance_assessment\": \"The t-value for the intercept is 36.838 and for 'Days' is 6.771. Although the output doesn't directly provide p-values, these t-values can be used to assess statistical significance. Given the large t-values, especially for 'Days', it's highly likely that the effect of days of sleep deprivation on reaction time is statistically significant. The standard error for 'Days' is 1.546, which provides a measure of the precision of the estimated effect of sleep deprivation. Packages like `lmerTest` can be used with this model to obtain p-values for these fixed effects estimates using methods like Satterthwaite or Kenward-Roger approximations.\", #>   \"goodness_of_fit\": \"The REML criterion at convergence is 1743.6. This value is useful for comparing nested models fitted to the same data. Lower REML values indicate a better fit, but only when comparing models with the same fixed effects. Other metrics like AIC and BIC, which can be obtained from the model, are better suited to non-nested model comparisons.\", #>   \"assumptions_check\": \"Several assumptions should be checked to ensure the validity of this model. First, examine the normality of the residuals using a Q-Q plot or histogram of `resid(object)`. Second, assess homoscedasticity by plotting residuals against fitted values (`fitted(object)`), looking for constant variance. Third, check the normality of the random effects using Q-Q plots of the random effects (`ranef(object)`). Additionally, linearity of the fixed effect 'Days' should be checked, possibly by plotting residuals against 'Days'. Finally, it's important to check for influential observations that might disproportionately affect the model results.\", #>   \"key_findings\": \"- On average, baseline reaction time (Day 0) is estimated to be approximately 251.4 ms.\\n- Each additional day of sleep deprivation is associated with an average increase of approximately 10.5 ms in reaction time.\\n- There is substantial inter-individual variability in both baseline reaction time (Std.Dev. = 24.741 ms) and the effect of sleep deprivation (Std.Dev. = 5.922 ms) across subjects.\\n- The correlation between random intercepts and slopes is low (0.07), suggesting a weak relationship between a subject's baseline reaction time and how their reaction time changes with sleep deprivation.\", #>   \"warnings_limitations\": \"The interpretation relies on the assumption that the model is correctly specified and that the assumptions of linear mixed-effects models are met. The absence of p-values in the base output from `lmer` should be addressed using appropriate packages like `lmerTest` to draw firmer conclusions about statistical significance. Also, remember that correlation does not equal causation; while the model suggests a relationship between sleep deprivation and reaction time, it does not prove that sleep deprivation *causes* the change in reaction time.\" #> } #> ```"},{"path":"/articles/statlingua.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"The statlingua Package","text":"statlingua package, working hand--hand ellmer, provides powerful user-friendly bridge interpretive capabilities Large Language Models R users. mastering explain() function arguments—especially context, audience, verbosity, style—can transform standard statistical outputs rich, understandable narratives tailored needs. Important Considerations: * quality LLM’s explanation heavily influenced clarity context provide inherent capabilities LLM choose (vignette, ’ve focused Google Gemini). * LLM Output Variability: statlingua uses detailed prompts guide LLM towards desired output style content, nature generative AI means responses can vary. requested style aim, statlingua includes measures clean output (like removing language fences), exact formatting content may always perfectly consistent across repeated calls different LLM versions. Always critically review generated explanations. * style = \"json\" option, requests JSON output, ensure jsonlite package available intend parse JSON string R list within session. Remember critically review explanations generated LLM. Happy explaining!","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Brandon M. Greenwell. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greenwell BM (2025). statlingua: Explain Statistical Output Large Language Models. https://github.com/bgreenwell/statlingua, https://bgreenwell.github.io/statlingua/.","code":"@Manual{,   title = {statlingua: Explain Statistical Output with Large Language Models},   author = {Brandon M. Greenwell},   year = {2025},   note = {https://github.com/bgreenwell/statlingua, https://bgreenwell.github.io/statlingua/}, }"},{"path":"/index.html","id":"statlingua-","dir":"","previous_headings":"","what":"Explain Statistical Output with Large Language Models","title":"Explain Statistical Output with Large Language Models","text":"statlingua R package designed help bridge gap complex statistical outputs clear, human-readable explanations. leveraging power Large Language Models (LLMs), statlingua helps effortlessly translate dense jargon statistical models—coefficients, p-values, model fit indices, —straightforward, context-aware natural language. Whether ’re student grappling new statistical concepts, researcher needing communicate findings broader audience, data scientist looking quickly draft reports, statlingua makes statistical journey smoother accessible.","code":""},{"path":"/index.html","id":"why-statlingua","dir":"","previous_headings":"","what":"Why statlingua?","title":"Explain Statistical Output with Large Language Models","text":"Statistical models powerful, outputs can intimidating. statlingua empowers : Democratize Understanding: Make complex analyses accessible individuals varying levels statistical expertise. Enhance Learning & Education: Students can gain deeper intuition model outputs, connecting theory practical application. Use interactive learning aid demystify statistical concepts. Foster Interdisciplinary Collaboration: Researchers diverse fields can easily interpret discuss analytical results, leading richer insights. Streamline Reporting & Consulting: Quickly generate initial drafts interpretations reports presentations, saving time ensuring clarity clients stakeholders. Drive Data-Informed Decisions: Business professionals can better grasp statistical findings, enabling confident data-driven decision-making without needing become statistical experts . Accelerate Prototyping & Exploration: Rapidly understand model summaries iterative data exploration, allowing faster assessment refinement analyses. providing clear contextualized explanations, statlingua helps focus implications findings rather getting bogged technical minutiae.","code":""},{"path":"/index.html","id":"supported-models","dir":"","previous_headings":"","what":"Supported Models","title":"Explain Statistical Output with Large Language Models","text":"now, statlingua explicitly supports variety common statistical models R, including: Objects class \"htest\" (e.g., t.test(), prop.test()). Linear models (lm()) Generalized Linear Models (glm()). Linear Generalized Linear Mixed-Effects Models packages nlme (lme()) lme4 (lmer(), glmer()). Generalized Additive Models (gam() package mgcv). Survival Regression Models (survreg(), coxph() package survival). Proportional Odds Logistic Regression (polr() package MASS). Decision Trees (rpart() package rpart). …, robust default method model types!","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Explain Statistical Output with Large Language Models","text":"statlingua yet CRAN, can install development version GitHub: ’ll also need install ellmer package, can obtain CRAN:","code":"if (!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"bgreenwell/statlingua\") install.packages(\"ellmer\")  # >= 0.2.0"},{"path":"/index.html","id":"api-key-setup--ellmer-dependency","dir":"","previous_headings":"","what":"API Key Setup & ellmer Dependency","title":"Explain Statistical Output with Large Language Models","text":"statlingua doesn’t directly handle API keys LLM communication. acts sophisticated prompt engineering toolkit prepares inputs passes ellmer. ellmer package responsible interfacing various LLM providers (e.g., OpenAI, Google AI Studio, Anthropic). Please refer ellmer package documentation detailed instructions : Setting API keys (usually environment variables like OPENAI_API_KEY, GEMINI_API_KEY, etc.). Specifying different LLM models providers. configuration model parameter options. ellmer installed access LLM provider, statlingua seamlessly leverage connection.","code":""},{"path":"/index.html","id":"quick-example-explaining-a-linear-model","dir":"","previous_headings":"","what":"Quick Example: Explaining a Linear Model","title":"Explain Statistical Output with Large Language Models","text":"examples, including output, see introductory vignette.","code":"# Ensure you have an appropriate API key set up first! # Sys.setenv(GEMINI_API_KEY = \"<YOUR_API_KEY_HERE>\")   library(statlingua)  # Fit a polynomial regression model fm_cars <- lm(dist ~ poly(speed, degree = 2), data = cars) summary(fm_cars)  # Define some context (highly recommended!) cars_context <- \" This model analyzes the 'cars' dataset from the 1920s. Variables include:   * 'dist' - The distance (in feet) taken to stop.   * 'speed' - The speed of the car (in mph). We want to understand how speed affects stopping distance in the model. \"  # Establish connection to an LLM provider (in this case, Google Gemini) client <- ellmer::chat_google_gemini(echo = \"none\")  # defaults to gemini-2.0-flash  # Get an explanation explain(   fm_cars,                 # model for LLM to interpret/explain   client = client,         # connection to LLM provider   context = cars_context,  # additional context for LLM to consider   audience = \"student\",    # target audience   verbosity = \"detailed\",  # level of detail   style = \"markdown\"       # output style )  # Ask a follow-up question client$chat(   \"How can I construct confidence intervals for each coefficient in the model?\" )"},{"path":"/index.html","id":"extending-statlingua-to-support-new-models","dir":"","previous_headings":"","what":"Extending statlingua to Support New Models","title":"Explain Statistical Output with Large Language Models","text":"One statlingua’s core strengths extensibility. can add customize support new statistical model types crafting specific prompt components. system prompt sent LLM dynamically assembled several markdown files located inst/prompts/ directory package. main function explain() uses S3 dispatch. explain(my_model_object, ...) called, R looks method like explain.class_of_my_model_object(). found, explain.default() used.","code":""},{"path":"/index.html","id":"prompt-directory-structure","dir":"","previous_headings":"Extending statlingua to Support New Models","what":"Prompt Directory Structure","title":"Explain Statistical Output with Large Language Models","text":"prompts organized follows within inst/prompts/: role_base.md: Defines fundamental role LLM. caution.md: general cautionary note appended explanations. audience/: Markdown files different target audiences (e.g., novice.md, researcher.md). filename (e.g., “novice”) matches audience argument explain(). verbosity/: Markdown files different verbosity levels (e.g., brief.md, detailed.md). filename matches verbosity argument. style/: Markdown files defining output format (e.g., markdown.md, json.md). filename matches style argument. instructions.md: primary instructions explaining specific model type. tells LLM look model output, interpret , assumptions discuss. role_specific.md (Optional): Additional role details specific model type, augmenting common/role_base.md.","code":""},{"path":"/index.html","id":"example-adding-support-for-vglm-from-the-vgam-package","dir":"","previous_headings":"Extending statlingua to Support New Models","what":"Example: Adding Support for vglm from the VGAM package","title":"Explain Statistical Output with Large Language Models","text":"Let’s imagine want add dedicated support vglm (Vector Generalized Linear Models) objects VGAM package. Create New Prompt Files: create new directory inst/prompts/models/vglm/. Inside directory, ’d add: inst/prompts/models/vglm/instructions.md: file contain detailed instructions LLM interpret vglm objects. ’d detail aspects summary(vglm_object) important, discuss coefficients (potentially multiple linear predictors), link functions, model fit statistics specific vglm, relevant assumptions. inst/prompts/models/vglm/role_specific.md (Optional): vglm models require LLM adopt slightly specialized persona. particular expertise Vector Generalized Linear Models (VGLMs), understanding diverse applications complex response types. Implement S3 Method: Add S3 method explain.vglm R script (e.g., R/explain_vglm.R): summarize.vglm method might also need implemented R/summarize.R summary(object) vglm needs special capture formatting LLM. utils::capture.output(summary(object)) sufficient, summarize.default might work initially. Add NAMESPACE Document: Ensure new method exported NAMESPACE file (usually handled roxygen2): S3method(explain, vglm) Add roxygen2 documentation blocks explain.vglm. Testing: Thoroughly test various vglm examples. might need iterate instructions.md role_specific.md refine LLM’s explanations. following pattern, statlingua can systematically extended cover vast array statistical models R!","code":"You are explaining a **Vector Generalized Linear Model (VGLM)** (from `VGAM::vglm()`).  **Core Concepts & Purpose:** VGLMs are highly flexible, extending GLMs to handle multiple linear predictors and a wider array of distributions and link functions, including multivariate responses. Identify the **Family** (e.g., multinomial, cumulative) and **Link functions**.  **Interpretation:** * **Coefficients:** Explain for each linear predictor. Pay attention to link functions (e.g., log odds, log relative risk). Clearly state reference categories. * **Model Fit:** Discuss deviance, AIC, etc. * **Assumptions:** Mention relevant assumptions. #' Explain a vglm object #' #' @inheritParams explain #' @param object A \\code{vglm} object. #' @export explain.vglm <- function(     object,     client,     context = NULL,     audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),     verbosity = c(\"moderate\", \"brief\", \"detailed\"),     style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),     ...   ) {   audience <- match.arg(audience)   verbosity <- match.arg(verbosity)   style <- match.arg(style)    # Use the internal .explain_core helper if it suits,   # or implement custom logic if vglm needs special handling.   # .explain_core handles system prompt assembly, user prompt building,   # and calling the LLM via the client.   # 'name' should match the directory name in inst/prompts/models/   # 'model_description' is what's shown to the user in the prompt.   .explain_core(     object = object,     client = client,     context = context,     audience = audience,     verbosity = verbosity,     style = style,     name = \"vglm\", # This tells .assemble_sys_prompt to look in inst/prompts/models/vglm/     model_description = \"Vector Generalized Linear Model (VGLM) from VGAM\"   ) }"},{"path":"/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Explain Statistical Output with Large Language Models","text":"Contributions welcome! Please see GitHub issues areas can help.","code":""},{"path":"/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Explain Statistical Output with Large Language Models","text":"statlingua available GNU General Public License v3.0 (GNU GPLv3). See LICENSE.md file details.","code":""},{"path":"/reference/explain.html","id":null,"dir":"Reference","previous_headings":"","what":"Explain statistical output — explain","title":"Explain statistical output — explain","text":"Use LLM explain output various statistical objects using straightforward, understandable, context-aware natural language descriptions.","code":""},{"path":"/reference/explain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explain statistical output — explain","text":"","code":"explain(   object,   client,   context = NULL,   audience = c(\"novice\", \"student\", \"researcher\", \"manager\", \"domain_expert\"),   verbosity = c(\"moderate\", \"brief\", \"detailed\"),   style = c(\"markdown\", \"html\", \"json\", \"text\", \"latex\"),   ... )  # Default S3 method explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'htest' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'lm' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'glm' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'polr' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'lme' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'lmerMod' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'glmerMod' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'gam' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'survreg' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'coxph' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )  # S3 method for class 'rpart' explain(   object,   client,   context = NULL,   audience = \"novice\",   verbosity = \"moderate\",   style = \"markdown\",   ... )"},{"path":"/reference/explain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explain statistical output — explain","text":"object appropriate statistical object. example, object can output calling t.test() glm(). client Chat object (e.g., calling chat_openai() [chat_gemini()][ellmer::chat_gemini)]). [ellmer::chat_gemini)]: R:ellmer::chat_gemini) context Optional character string providing additional context, background research question information data. audience Character string indicating target audience: \"novice\" - Assumes user limited statistics background (default). \"student\" - Assumes user learning statistics. \"researcher\" - Assumes user strong statistical background familiar common methodologies. \"manager\" - Assumes user needs high-level insights decision-making. \"domain_expert\" - Assumes user expert field necessarily statistics. verbosity Character string indicating desired verbosity: \"moderate\" - Offers balanced explanation (default). \"brief\" - Offers high-level summary. \"detailed\" - Offers comprehensive interpretation. style Character string indicating desired output style: \"markdown\" (default) - Output formatted plain Markdown. \"html\" - Output formatted HTML fragment. \"json\" - Output structured JSON string parseable R list. \"text\" - Output plain text. \"latex\" - Output LaTeX fragment. ... Additional optional arguments. (Currently ignored.)","code":""},{"path":"/reference/explain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explain statistical output — explain","text":"object class \"statlingua_explanation\". Essentially list following components: text - Character string representation LLM's response. model_type - Character string giving model type (e.g., \"lm\" \"coxph\"). audience - Character string specifying level intended audience explanations. verbosity - Character string specifying level verbosity level detail provided explanation.","code":""},{"path":"/reference/explain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explain statistical output — explain","text":"","code":"if (FALSE) { # \\dontrun{ # Polynomial regression fm1 <- lm(dist ~ poly(speed, degree = 2), data = cars) context <- \" The data give the speed of cars (mph) and the distances taken to stop (ft). Note that the data were recorded in the 1920s! \" # Use Google Gemini to explain the output; requires an API key; see # ?ellmer::chat_google_gemini for details client <- ellmer::chat_google_gemini(echo = \"none\") ex <- explain(fm1, client = client, context = context)  # Poisson regression example from ?stats::glm counts <- c(18,17,15,20,10,20,25,13,12) outcome <- gl(3,1,9) treatment <- gl(3,3) data.frame(treatment, outcome, counts) # showing data fm2 <- glm(counts ~ outcome + treatment, family = poisson())  # Use Google Gemini to explain the output; requires an API key; see # ?ellmer::chat_google_gemini for details client <- ellmer::chat_google_gemini() explain(fm2, client = client, audience = \"student\", verbosity = \"detailed\") } # }"},{"path":"/reference/print.statlingua_explanation.html","id":null,"dir":"Reference","previous_headings":"","what":"Print LLM explanation — print.statlingua_explanation","title":"Print LLM explanation — print.statlingua_explanation","text":"Print formatted version LLMs explanation using cat().","code":""},{"path":"/reference/print.statlingua_explanation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print LLM explanation — print.statlingua_explanation","text":"","code":"# S3 method for class 'statlingua_explanation' print(x, ...)"},{"path":"/reference/print.statlingua_explanation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print LLM explanation — print.statlingua_explanation","text":"x statlingua_explanation object. ... Additional optional arguments passed print.default().","code":""},{"path":"/reference/print.statlingua_explanation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print LLM explanation — print.statlingua_explanation","text":"Invisibly returns printed statlingua_explanation object.","code":""},{"path":"/reference/summarize.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize statistical output — summarize","title":"Summarize statistical output — summarize","text":"Generate text-based summaries statistical output can embedded prompts querying Large Language Models (LLMs). Intended primarily internal use.","code":""},{"path":"/reference/summarize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize statistical output — summarize","text":"","code":"summarize(object, ...)  # Default S3 method summarize(object, ...)  # S3 method for class 'htest' summarize(object, ...)  # S3 method for class 'lm' summarize(object, ...)  # S3 method for class 'glm' summarize(object, ...)  # S3 method for class 'polr' summarize(object, ...)  # S3 method for class 'lme' summarize(object, ...)  # S3 method for class 'lmerMod' summarize(object, ...)  # S3 method for class 'glmerMod' summarize(object, ...)  # S3 method for class 'gam' summarize(object, ...)  # S3 method for class 'survreg' summarize(object, ...)  # S3 method for class 'coxph' summarize(object, ...)  # S3 method for class 'rpart' summarize(object, ...)"},{"path":"/reference/summarize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize statistical output — summarize","text":"object object summary desired (e.g., glm object). ... Additional optional arguments. (Currently ignored.)","code":""},{"path":"/reference/summarize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize statistical output — summarize","text":"character string summarizing statistical output.","code":""},{"path":[]},{"path":"/reference/summarize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize statistical output — summarize","text":"","code":"tt <- t.test(1:10, y = c(7:20)) summarize(tt)  # prints output as a character string #> [1] \"\\n\\tWelch Two Sample t-test\\n\\ndata:  1:10 and c(7:20)\\nt = -5.4349, df = 21.982, p-value = 1.855e-05\\nalternative hypothesis: true difference in means is not equal to 0\\n95 percent confidence interval:\\n -11.052802  -4.947198\\nsample estimates:\\nmean of x mean of y \\n      5.5      13.5 \\n\" cat(summarize(tt))  # more useful for reading #>  #> \tWelch Two Sample t-test #>  #> data:  1:10 and c(7:20) #> t = -5.4349, df = 21.982, p-value = 1.855e-05 #> alternative hypothesis: true difference in means is not equal to 0 #> 95 percent confidence interval: #>  -11.052802  -4.947198 #> sample estimates: #> mean of x mean of y  #>       5.5      13.5"}]
